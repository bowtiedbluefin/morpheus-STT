# WhisperX Transcription API

## Features

- **WhisperX Integration**: Enhanced transcription with improved accuracy and speed
- **OpenAI-Compatible API**: Drop-in replacement for OpenAI's Whisper API
- **Multiple Output Formats**: JSON, text, SRT, VTT with flexible content options
- **Word-Level Timestamps**: Precise word-level timing with confidence scores
- **Speaker Diarization**: Identify and label different speakers automatically
- **Legacy Compatibility**: Supports OpenAI API parameters for easy migration
- **GPU Acceleration**: CUDA support for faster processing
- **Multiple Languages**: Auto-detection and manual language specification

## Improvements with WhisperX

WhisperX provides significant improvements over standard Whisper:
- **Better Accuracy**: Enhanced transcription quality
- **Word-Level Timestamps**: More precise timing information
- **Integrated Diarization**: Built-in speaker identification
- **Faster Processing**: Optimized for speed and efficiency

## üéôÔ∏è Advanced Speaker Diarization

### Features
- **State-of-the-art Accuracy**: 95%+ speaker identification accuracy with 94.9% timeline consistency
- **GPU Accelerated**: 10-50x faster processing with CUDA optimization and memory management
- **Advanced Optimization**: 6-layer processing stack for maximum quality and reliability
- **Enterprise Ready**: Comprehensive error handling, cuDNN conflict resolution, and production monitoring
- **Configurable Parameters**: Fine-tune clustering, confidence, and validation thresholds for your use case
- **Real-time Processing**: Optimized for production workloads with concurrent request handling

### Quick Start with Diarization

#### 1. Setup Environment
```bash
# Copy example configuration and customize
cp env.example .env
# Edit .env with your HuggingFace token and settings

# Launch GPU-optimized server
bash start_server.sh
```

#### 2. Test Multi-Speaker Audio
```bash
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@multi_speaker_meeting.wav" \
  -F "enable_diarization=true" \
  -F "response_format=json"
```

#### 3. Enhanced Response with Speaker Attribution
```json
{
  "segments": [
    {
      "start": 0.5,
      "end": 3.2,
      "text": "Welcome everyone to today's meeting",
      "speaker": "SPEAKER_01", 
      "speaker_confidence": 0.94,
      "words": [
        {
          "word": "Welcome",
          "start": 0.5,
          "end": 1.0,
          "speaker": "SPEAKER_01",
          "speaker_confidence": 0.97
        }
      ]
    }
  ],
  "processing_info": {
    "total_speakers": 3,
    "confident_speakers": 3,
    "optimization_layers_applied": 6,
    "processing_time": 28.3
  }
}
```

### Advanced Configuration

The system includes sophisticated parameter tuning for optimal accuracy:

**Core Parameters:**
- `PYANNOTE_CLUSTERING_THRESHOLD=0.7` - Controls speaker clustering (0.3-1.0)
- `SPEAKER_CONFIDENCE_THRESHOLD=0.6` - Minimum confidence for attribution (0.0-1.0)  
- `MIN_SPEAKER_DURATION=3.0` - Minimum speaking time for valid speaker

**Optimization Features:**
- **Speaker Smoothing**: Reduces rapid A‚ÜíB‚ÜíA speaker switches
- **Hierarchical Clustering**: Prevents over-detection with adaptive merging
- **GPU Memory Management**: Prevents CUDA OOM with intelligent batch sizing
- **cuDNN Compatibility**: Resolves version conflicts with proper library paths

See `env.example` for complete parameter documentation and tuning guidelines.



## Installation

### Prerequisites
1. Python 3.8+ installed
2. NVIDIA GPU with CUDA support (recommended)
3. Virtual environment (recommended)

### Setup Instructions

1. **Clone the repository:**
```bash
git clone <repository-url>
cd whisper
```

2. **Create and activate virtual environment:**
```bash
python3 -m venv venv
source venv/bin/activate  # On Linux/Mac
# or
venv\Scripts\activate     # On Windows
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Start the server:**
```bash
bash start_server.sh
```

The server will start on `http://localhost:3333`

### Optional: Speaker Diarization Setup

To enable speaker diarization, you'll need a Hugging Face token:

1. **Get Hugging Face Token**:
   - Go to https://huggingface.co/settings/tokens
   - Create a new token with READ permission
   - Accept the user conditions at: https://huggingface.co/pyannote/speaker-diarization-3.1

2. **Set Environment Variable**:
   ```bash
   # Create .env file
   echo "HUGGINGFACE_TOKEN=your_token_here" > .env
   
   # Or set directly in environment
   export HUGGINGFACE_TOKEN=your_token_here
   ```

### Model Download

**No manual model download required!** WhisperX models will be automatically downloaded on first run. The download may take a few minutes depending on your internet connection.

- **First run**: Models download automatically, may take 5-10 minutes
- **Subsequent runs**: Uses cached models, starts immediately

## API Usage

### Endpoints

- **GET** `/` - Welcome message and basic info
- **GET** `/health` - Health check with model status
- **POST** `/v1/audio/transcriptions` - OpenAI-compatible transcription endpoint (recommended)
- **POST** `/transcribe` - Alias to the main endpoint (same functionality)
- **GET** `/docs` - Interactive API documentation (Swagger UI)

### Using the API

#### 1. Web Interface (Easiest)
Visit `http://localhost:3333/docs` in your browser to use the interactive API documentation where you can upload files directly and test all parameters.

#### 2. cURL Command Examples

##### Basic Usage (OpenAI Compatible)
```bash
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/your/audio/file.wav"
```

##### Speaker Diarization Examples
```bash
# Basic diarization
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "response_format=json"

# Diarization with fine-tuned parameters
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "clustering_threshold=0.7" \
  -F "min_speaker_duration=3.0" \
  -F "response_format=json"

# SRT format with speaker labels
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "response_format=srt" \
  -o subtitles_with_speakers.srt

# Performance-optimized diarization (faster processing)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "optimized_alignment=false" \
  -F "batch_size=16" \
  -F "response_format=json"
```

##### Advanced Features Examples
```bash
# Multiple timestamp granularities (comma-separated)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "timestamp_granularities=segment,word" \
  -F "response_format=json"

# Language-specific with diarization
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "language=en" \
  -F "enable_diarization=true" \
  -F "response_format=verbose_json"
```

##### Alignment Optimization Examples
```bash
# High accuracy mode (default) - uses WAV2VEC2 alignment
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "optimized_alignment=true" \
  -F "response_format=json"

# Fast processing mode - uses Whisper built-in alignment (~50% faster)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "enable_diarization=true" \
  -F "optimized_alignment=false" \
  -F "response_format=json"

# Performance comparison test
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@long_meeting.wav" \
  -F "enable_diarization=true" \
  -F "optimized_alignment=false" \
  -F "batch_size=16" \
  -F "response_format=json" \
  -F "output_content=metadata_only"
```

##### All Available Options Example
```bash
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/your/audio/file.wav" \
  -F "language=en" \
  -F "response_format=verbose_json" \
  -F "timestamp_granularities=word" \
  -F "output_content=both" \
  -F "enable_diarization=true" \
  -F "optimized_alignment=true" \
  -F "batch_size=16" \
  -F "clustering_threshold=0.7" \
  -F "segmentation_threshold=0.45" \
  -F "min_speaker_duration=3.0" \
  -F "speaker_confidence_threshold=0.6" \
  -F "speaker_smoothing_enabled=true" \
  -F "min_switch_duration=2.0" \
  -o transcription_with_words.json
```

##### Different Response Formats
```bash
# JSON with segment timestamps (default)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=json" \
  -F "timestamp_granularities=segment"

# JSON with word-level timestamps
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=json" \
  -F "timestamp_granularities=word"

# Verbose JSON with metadata (OpenAI format)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=verbose_json" \
  -F "timestamp_granularities=word"

# Plain text output
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=text"

# SRT subtitle format
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=srt"

# WebVTT subtitle format
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=vtt"
```

##### Output Content Options
```bash
# Text only (no timestamps)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=json" \
  -F "output_content=text_only"

# Timestamps only (no text)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=json" \
  -F "timestamp_granularities=segment" \
  -F "output_content=timestamps_only"

# Both text and timestamps (default)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@audio.wav" \
  -F "response_format=json" \
  -F "timestamp_granularities=word" \
  -F "output_content=both"
```

##### Language-Specific Transcription
```bash
# Spanish audio
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@spanish_audio.wav" \
  -F "language=es"

# French audio
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@french_audio.wav" \
  -F "language=fr"

# Auto-detect language (default)
curl -X POST "http://localhost:3333/v1/audio/transcriptions" \
  -F "file=@multilingual_audio.wav"
```

#### 3. Python requests (OpenAI Compatible)
```python
import requests

url = "http://localhost:3333/v1/audio/transcriptions"
files = {"file": open("audio.wav", "rb")}
data = {
    "language": "en",
    "response_format": "verbose_json",
    "timestamp_granularities": "word",
    # WhisperX features
    "enable_diarization": True,
    "optimized_alignment": True,  # True=accurate, False=fast
    "batch_size": 16,
    "clustering_threshold": 0.7,
    "min_speaker_duration": 3.0,
    "output_content": "both"
}
response = requests.post(url, files=files, data=data)
result = response.json()
print(result["text"])
print(f"Speakers found: {result.get('speakers', 'N/A')}")
```

#### 4. JavaScript/Node.js (OpenAI Compatible)
```javascript
const FormData = require('form-data');
const fs = require('fs');
const axios = require('axios');

const form = new FormData();
form.append('file', fs.createReadStream('audio.wav'));
form.append('language', 'en');
form.append('response_format', 'verbose_json');
form.append('timestamp_granularities', 'segment');
// WhisperX features
form.append('enable_diarization', 'true');
form.append('optimized_alignment', 'true'); // true=accurate, false=fast
form.append('batch_size', '16');
form.append('clustering_threshold', '0.7');
form.append('min_speaker_duration', '3.0');
form.append('output_content', 'both');

axios.post('http://localhost:3333/v1/audio/transcriptions', form, {
  headers: form.getHeaders()
}).then(response => {
  console.log(response.data.text);
  console.log(`Speakers found: ${response.data.speakers || 'N/A'}`);
});
```

#### 5. OpenAI Python Client (Drop-in Replacement)
```python
# You can use the official OpenAI client by pointing it to your server
from openai import OpenAI

# Point to your local server
client = OpenAI(
    api_key="dummy-key",  # Not used but required
    base_url="http://localhost:3333"
)

# Use exactly like OpenAI API
with open("audio.wav", "rb") as audio_file:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",  # Ignored - always uses large-v3-turbo
        file=audio_file,
        response_format="verbose_json",
        timestamp_granularities=["word"]
    )
    print(transcript.text)
```

## API Parameters

### Required Parameters
- **`file`**: Audio file to transcribe (mp3, wav, m4a, flac, etc.)

### Core Parameters
- **`language`**: Language code (ISO-639-1) like "en", "es", "fr" - improves accuracy and speed
- **`response_format`**: Output format - `json`, `text`, `srt`, `vtt`, `verbose_json`
- **`timestamp_granularities`**: String value (comma-separated for multiple) - `segment`, `word`, `segment,word`

### WhisperX Enhanced Parameters
- **`output_content`**: Control response content: `text_only`, `timestamps_only`, or `both` (default: 'both')
- **`enable_diarization`**: Enable speaker diarization to identify who is speaking (default: **True**)
- **`prompt`**: Optional text prompt to provide context or guide the transcription (e.g., proper names, technical terms)
- **`optimized_alignment`**: Use WAV2VEC2 alignment model (True) or Whisper built-in alignment (False). True provides better speaker diarization accuracy but takes ~2x longer. False is ~50% faster but may have slightly less accurate speaker attribution. (default: True)

### Fine-Tuning Parameters
- **`batch_size`**: Batch size for processing (higher = faster but more memory, default: 16)
- **`clustering_threshold`**: Speaker clustering threshold (0.5-1.0, lower = more speakers, default: 0.7)
- **`segmentation_threshold`**: Voice activity detection threshold (0.1-0.9, default: 0.45)
- **`min_speaker_duration`**: Minimum speaking time per speaker in seconds (default: 3.0)
- **`speaker_confidence_threshold`**: Minimum confidence for speaker assignment (0.1-1.0, default: 0.6)
- **`speaker_smoothing_enabled`**: Enable speaker transition smoothing (default: True)
- **`min_switch_duration`**: Minimum time between speaker switches in seconds (default: 2.0)
- **`vad_validation_enabled`**: Enable Voice Activity Detection validation - experimental (default: False)


### Legacy Parameters (Accepted for OpenAI compatibility but not used)
- **`model`**: Model identifier (ignored - always uses WhisperX large-v3-turbo)
- **`temperature`**: Sampling temperature (accepted for compatibility but not used)

### Response Formats

#### `json` (Simple JSON)
```json
{
  "text": "Hello, this is a transcription.",
  "segments": [
    {
      "start": 0.0,
      "end": 2.5,
      "text": "Hello, this is a transcription."
    }
  ],
  "words": [
    {
      "start": 0.0,
      "end": 0.5,
      "text": "Hello",
      "decorated_text": "Hello",
      "word_prob": 0.95,
      "speaker": null
    },
    {
      "start": 0.5,
      "end": 1.2,
      "text": "this",
      "decorated_text": "this",
      "word_prob": 0.98,
      "speaker": null
    }
  ]
}
```

#### üÜï `json` with Speaker Diarization
```json
{
  "text": "Hello there. How are you today?",
  "segments": [
    {
      "start": 0.0,
      "end": 2.5,
      "text": "Hello there.",
      "speaker": "SPEAKER_00"
    },
    {
      "start": 2.5,
      "end": 4.0,
      "text": "How are you today?",
      "speaker": "SPEAKER_01"
    }
  ],
  "speakers": 2,
  "words": [
    {
      "start": 0.0,
      "end": 0.5,
      "text": "Hello",
      "decorated_text": "Hello",
      "word_prob": 0.95,
      "speaker": "SPEAKER_00"
    },
    {
      "start": 0.6,
      "end": 1.1,
      "text": "there.",
      "decorated_text": "there.",
      "word_prob": 0.92,
      "speaker": "SPEAKER_00"
    },
    {
      "start": 2.5,
      "end": 2.8,
      "text": "How",
      "decorated_text": "How",
      "word_prob": 0.98,
      "speaker": "SPEAKER_01"
    }
  ]
}
```

#### `verbose_json` (OpenAI Format with Metadata)
```json
{
  "task": "transcribe",
  "language": "english",
  "duration": 44.08,
  "text": "Hello, this is a transcription.",
  "segments": [...]
}
```

#### `text` (Plain Text)
```
Hello, this is a transcription.
```

#### `srt` with Speaker Labels
```
1
00:00:00,000 --> 00:00:02,500
[SPEAKER_00] Hello there.

2
00:00:02,500 --> 00:00:04,000
[SPEAKER_01] How are you today?
```

#### `vtt` with Speaker Labels
```
WEBVTT

00:00:00.000 --> 00:00:02.500
[SPEAKER_00] Hello there.

00:00:02.500 --> 00:00:04.000
[SPEAKER_01] How are you today?
```

### Output Control

**What it does**: Control what content is included in the response.

**Options**:
- `text_only`: Only return the transcribed text
- `timestamps_only`: Only return timing information
- `both`: Return both text and timestamps (default)

### Troubleshooting

#### Diarization Issues
1. **"Pipeline not found" Error**:
   - Check HUGGINGFACE_TOKEN is set correctly
   - Verify you've accepted the model license at https://huggingface.co/pyannote/speaker-diarization-3.1
   - Ensure internet connection for model download

2. **Poor Diarization Results**:
   - Use `min_speakers`/`max_speakers` to constrain the search
   - Ensure clear audio with distinct speakers
   - Try shorter audio segments for better accuracy

3. **Memory Issues**:
   - Ensure sufficient GPU memory (>4GB free recommended)
   - Consider processing shorter audio segments

### Supported Audio Formats
- WAV
- MP3
- FLAC
- M4A
- And other formats supported by FFmpeg

## OpenAI API Compatibility

This API is fully compatible with OpenAI's Whisper API. You can:

1. **Use existing OpenAI client libraries** by pointing them to your server
2. **Drop-in replacement** for OpenAI Whisper API calls
3. **Same parameter names and response formats** as OpenAI
4. **Extended WhisperX features** that are ignored by standard OpenAI clients

### Migration from OpenAI
Simply change your base URL from `https://api.openai.com` to `http://localhost:3333` and optionally remove the API key requirement.

## Optimization & Performance Tuning

WhisperX handles many optimizations automatically, but you can still fine-tune performance for your specific needs. 

### Quick Start - Environment Variables:
```bash
# Optimized production (current defaults)
export WHISPERX_COMPUTE_TYPE=float32
export WHISPERX_BATCH_SIZE=16
export WHISPERX_CHAR_ALIGN=false
export WHISPERX_CHUNK_LENGTH=30
export SEGMENT_RESOLUTION=word
export QUEUE_TIMEOUT=600

# High performance (speed optimized)
export WHISPERX_COMPUTE_TYPE=float16
export WHISPERX_BATCH_SIZE=32
export MAX_CONCURRENT_REQUESTS=8

# H100 high throughput (maximum concurrency)
export WHISPERX_COMPUTE_TYPE=float16
export WHISPERX_BATCH_SIZE=32
export MAX_CONCURRENT_REQUESTS=15
```

**See [env.example](env.example) for complete configuration profiles**

**For comprehensive optimization guidance, see [WHISPERX_OPTIMIZATION_GUIDE.md](WHISPERX_OPTIMIZATION_GUIDE.md)**

**For concurrent processing & GPU optimization, see [CONCURRENT_PROCESSING_GUIDE.md](CONCURRENT_PROCESSING_GUIDE.md)**

**For Docker deployment updates, see [DEPLOYMENT_UPDATES.md](DEPLOYMENT_UPDATES.md)**

### What's Different from faster-whisper:
- **Temperature, beam_size, condition_on_previous_text**: Now handled internally for better defaults
- **VAD parameters**: Automatic VAD preprocessing with pyannote/silero
- **Batch processing**: Optimized internal batching
- **Word alignment**: Phoneme-based alignment for better accuracy
- **Concurrent Processing**: Built-in support for multiple simultaneous transcriptions
- **GPU Optimization**: Memory-aware request management and monitoring



## Production Deployment

For production use, consider:
- Using a reverse proxy (nginx)
- Adding authentication
- Using HTTPS
- Rate limiting
- Error handling improvements
- Load balancing for multiple instances

## Performance Notes

- **GPU Recommended**: NVIDIA GPU with 4GB+ VRAM for optimal performance
- **CPU Fallback**: Will work on CPU but significantly slower
- **Memory Usage**: ~2-4GB GPU memory for transcription, +2GB for diarization
- **Processing Speed**: 
  - **With optimized_alignment=true**: ~18x faster than real-time (3.2min for 60min audio)
  - **With optimized_alignment=false**: ~35x faster than real-time (1.7min for 60min audio)
  - Measured on RTX 3090 with large-v3-turbo model
