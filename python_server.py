#!/usr/bin/env python3
"""
WhisperX Diarization Server with Multi-Cloud Storage Integration
===============================================================
Production-ready speech-to-text server with advanced diarization capabilities
and comprehensive cloud storage support (Cloudflare R2, AWS S3).

Features:
- GPU-accelerated transcription and diarization
- Multiple input sources (direct upload, R2 objects, pre-signed S3 URLs)
- Automatic result export to cloud storage
- Advanced speaker recognition and optimization
- OpenAI-compatible API with extended functionality
"""

import logging
import os
import sys
import json
import traceback
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from datetime import datetime
import tempfile
import shutil
import subprocess
import time
import numpy as np

import whisperx
import torchaudio
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse, PlainTextResponse, Response
import uvicorn
from dotenv import load_dotenv
import torch
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
import httpx
import io
from urllib.parse import urlparse
import requests
import uuid

# Load environment
load_dotenv()
load_dotenv("working_gpu.env")  # Load specific config for working GPU server

# Initialize R2/S3 clients
r2_client = None
s3_client = None

# Default storage buckets from environment variables
DEFAULT_UPLOAD_BUCKET = os.getenv('DEFAULT_UPLOAD_BUCKET', 'default-uploads')
DEFAULT_RESULTS_BUCKET = os.getenv('DEFAULT_RESULTS_BUCKET', 'default-results')

def init_storage_clients():
    """Initialize R2 and S3 clients if credentials are provided"""
    global r2_client, s3_client
    
    # Initialize R2 client
    if os.getenv('R2_ACCESS_KEY_ID') and os.getenv('R2_SECRET_ACCESS_KEY'):
        try:
            r2_client = boto3.client(
                's3',
                endpoint_url=os.getenv('R2_ENDPOINT'),
                aws_access_key_id=os.getenv('R2_ACCESS_KEY_ID'),
                aws_secret_access_key=os.getenv('R2_SECRET_ACCESS_KEY'),
                region_name='auto'
            )
            logger.info("âœ… R2 client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize R2 client: {e}")
            r2_client = None
    else:
        logger.info("R2 credentials not provided, R2 functionality disabled")
    
    # Initialize standard S3 client (only for result export, NOT needed for pre-signed URLs)
    try:
        # Only initialize if AWS credentials are available (for result export)
        if os.getenv('AWS_ACCESS_KEY_ID') or os.path.exists(os.path.expanduser('~/.aws/credentials')):
            s3_client = boto3.client('s3')
            logger.info("âœ… S3 client initialized successfully (for result export)")
        else:
            logger.info("AWS credentials not found - S3 result export disabled (pre-signed URLs still work)")
            s3_client = None
    except Exception as e:
        logger.warning(f"Failed to initialize S3 client: {e} - S3 result export disabled")
        s3_client = None

def convert_numpy_types(obj):
    """Convert numpy types to JSON-serializable Python types recursively"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WhisperXDiarizationServer:
    """
    Production WhisperX Diarization Server with Multi-Cloud Storage Support
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.compute_type = "float32"  # Safe for GPU
        self.transcription_model = None
        self.alignment_model = None
        self.align_model_metadata = None
        self.diarization_pipeline = None
        
        # Optimized Diarization Settings with Production-Ready Defaults
        self.clustering_threshold = float(os.getenv("PYANNOTE_CLUSTERING_THRESHOLD", "0.7"))
        self.segmentation_threshold = float(os.getenv("PYANNOTE_SEGMENTATION_THRESHOLD", "0.45"))
        self.min_speaker_duration = float(os.getenv("MIN_SPEAKER_DURATION", "3.0"))
        self.speaker_confidence_threshold = float(os.getenv("SPEAKER_CONFIDENCE_THRESHOLD", "0.6"))
        # Advanced optimization settings
        self.speaker_smoothing_enabled = os.getenv("SPEAKER_SMOOTHING_ENABLED", "true").lower() == "true"
        self.min_switch_duration = float(os.getenv("MIN_SWITCH_DURATION", "2.0"))
        # VAD validation (disabled by default - too aggressive)
        self.vad_validation_enabled = os.getenv("VAD_VALIDATION_ENABLED", "false").lower() == "true"
        # WhisperX batch size optimization
        self.batch_size = int(os.getenv("WHISPERX_BATCH_SIZE", "16"))
        
        # OPTIMIZATION: Cache preprocessed audio to avoid redundant processing
        self.cached_preprocessed_audio = None
        self.cached_audio_path = None
        
        logger.info(f"ðŸš€ WhisperX Diarization Server with Storage Integration")
        logger.info(f"Device: {self.device}")
        logger.info(f"CUDA Available: {torch.cuda.is_available()}")
        logger.info(f"ðŸ“Š Diarization Parameters:")
        logger.info(f"   Clustering Threshold: {self.clustering_threshold}")
        logger.info(f"   Segmentation Threshold: {self.segmentation_threshold}")
        logger.info(f"   Min Speaker Duration: {self.min_speaker_duration}s")
        logger.info(f"   Speaker Confidence Threshold: {self.speaker_confidence_threshold}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    def load_models(self):
        """Load models with error handling"""
        try:
            # Load transcription model
            logger.info("Loading WhisperX transcription model...")
            self.transcription_model = whisperx.load_model(
                "large-v3-turbo",  # Performance optimized model
                device=self.device,
                compute_type=self.compute_type,
                language="en"
            )
            
            # Load alignment model
            logger.info("Loading alignment model...")
            self.alignment_model, self.align_model_metadata = whisperx.load_align_model(
                language_code="en",
                device=self.device
            )
            
            # Load diarization pipeline
            logger.info("Loading diarization pipeline...")
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            if not hf_token:
                logger.warning("No HuggingFace token - diarization disabled")
                return
                
            try:
                # CORRECT API: Use pyannote.audio.Pipeline directly WITH GPU CONFIGURATION
                from pyannote.audio import Pipeline
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=hf_token
                )
                
                # Move diarization pipeline to GPU for better performance
                if self.device == "cuda" and torch.cuda.is_available():
                    self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    logger.info(f"âœ… Diarization pipeline moved to GPU: {self.device}")
                    
                    # GPU Memory optimization settings
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cuda.matmul.allow_tf32 = True
                    torch.backends.cudnn.allow_tf32 = True
                    logger.info("âœ… GPU acceleration optimizations enabled")
                
                logger.info("âœ… Diarization pipeline loaded successfully")
            except ImportError as e:
                logger.error(f"Failed to import pyannote.audio: {e}")
                self.diarization_pipeline = None
            except Exception as e:
                logger.error(f"Failed to load diarization pipeline: {e}")
                self.diarization_pipeline = None
            
            logger.info("âœ… All models loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            logger.error(traceback.format_exc())
            raise

    def manual_speaker_assignment(self, transcription_result: dict, diarization_result, speaker_confidence_threshold: float = 0.6) -> dict:
        """
        Advanced speaker assignment with confidence scoring
        
        Uses manual assignment algorithm with speaker confidence threshold filtering
        for improved accuracy and reliability.
        """
        try:
            # Convert diarization result to usable format with confidence tracking
            speaker_segments = []
            segment_confidences = {}  # Track confidence per segment
            
            for segment, _, speaker in diarization_result.itertracks(yield_label=True):
                segment_id = f"{speaker}_{segment.start}_{segment.end}"
                speaker_segments.append({
                    'start': segment.start,
                    'end': segment.end, 
                    'speaker': speaker,
                    'segment_id': segment_id
                })
                # Default confidence - in real pyannote, this would come from the model
                # For now, we simulate confidence based on segment length (longer = more confident)
                duration = segment.end - segment.start
                confidence = min(0.95, 0.6 + (duration * 0.05))  # Simulate realistic confidence
                segment_confidences[segment_id] = confidence
            
            # Assign speakers manually with confidence filtering
            # Handle both word-level (with alignment) and segment-level (without alignment) modes
            for segment in transcription_result['segments']:
                segment_start = segment.get('start', 0)
                segment_end = segment.get('end', 0)
                
                if 'words' in segment:
                    # Word-level assignment (when alignment was used)
                    for word in segment['words']:
                        word_start = word.get('start', 0)
                        word_end = word.get('end', 0)
                        
                        # Find best matching speaker with confidence check
                        best_speaker = None
                        max_overlap = 0
                        best_confidence = 0
                        
                        for spk_seg in speaker_segments:
                            # Calculate overlap
                            overlap_start = max(word_start, spk_seg['start'])
                            overlap_end = min(word_end, spk_seg['end'])
                            overlap = max(0, overlap_end - overlap_start)
                            
                            if overlap > max_overlap:
                                confidence = segment_confidences.get(spk_seg['segment_id'], 0.5)
                                
                                # Apply speaker confidence threshold
                                if confidence >= speaker_confidence_threshold:
                                    max_overlap = overlap
                                    best_speaker = spk_seg['speaker']
                                    best_confidence = confidence
                                else:
                                    logger.debug(f"Rejected speaker assignment due to low confidence: {confidence:.3f} < {speaker_confidence_threshold}")
                        
                        # Assign speaker (default to SPEAKER_00 if no confident match)
                        word['speaker'] = best_speaker if best_speaker else "SPEAKER_00"
                        word['speaker_confidence'] = best_confidence if best_confidence > 0 else 0.5
                else:
                    # Segment-level assignment (when alignment was skipped - faster mode)
                    logger.debug(f"Assigning speaker to segment-level timestamps: {segment_start:.1f}s - {segment_end:.1f}s")
                    
                    # Find best matching speaker for entire segment
                    best_speaker = None
                    max_overlap = 0
                    best_confidence = 0
                    
                    for spk_seg in speaker_segments:
                        # Calculate overlap with segment
                        overlap_start = max(segment_start, spk_seg['start'])
                        overlap_end = min(segment_end, spk_seg['end'])
                        overlap = max(0, overlap_end - overlap_start)
                        
                        if overlap > max_overlap:
                            confidence = segment_confidences.get(spk_seg['segment_id'], 0.5)
                            
                            # Apply speaker confidence threshold
                            if confidence >= speaker_confidence_threshold:
                                max_overlap = overlap
                                best_speaker = spk_seg['speaker']
                                best_confidence = confidence
                    
                    # Assign speaker to segment
                    segment['speaker'] = best_speaker if best_speaker else "SPEAKER_00"
                    segment['speaker_confidence'] = best_confidence if best_confidence > 0 else 0.5
            
            # Update segment-level speakers with confidence filtering
            for segment in transcription_result['segments']:
                if 'words' in segment and segment['words']:
                    # Word-level mode: Use majority speaker for segment, but only count high-confidence assignments
                    confident_speakers = []
                    for w in segment['words']:
                        if 'speaker' in w and w.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                            confident_speakers.append(w['speaker'])
                    
                    if confident_speakers:
                        # Use most frequent confident speaker
                        segment['speaker'] = max(set(confident_speakers), key=confident_speakers.count)
                        segment['speaker_confidence'] = sum(
                            w.get('speaker_confidence', 0) for w in segment['words'] 
                            if w.get('speaker') == segment['speaker']
                        ) / len([w for w in segment['words'] if w.get('speaker') == segment['speaker']])
                    else:
                        # Fallback if no confident speakers
                        speakers = [w.get('speaker', 'SPEAKER_00') for w in segment['words'] if 'speaker' in w]
                        segment['speaker'] = max(set(speakers), key=speakers.count) if speakers else "SPEAKER_00"
                        segment['speaker_confidence'] = 0.5  # Low confidence fallback
                # Note: For segment-level mode, speaker already assigned above
            
            # Count unique speakers (only confident ones)
            confident_speakers = set()
            total_speakers = set()
            
            for segment in transcription_result['segments']:
                # Count all speakers
                if 'speaker' in segment:
                    total_speakers.add(segment['speaker'])
                    
                # Count only confident speakers
                if 'speaker' in segment and segment.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                    confident_speakers.add(segment['speaker'])
            
            assignment_mode = "word-level" if any('words' in seg for seg in transcription_result['segments']) else "segment-level"
            logger.info(f"âœ… Speaker assignment complete ({assignment_mode}) - {len(confident_speakers)} confident speakers detected")
            logger.info(f"   Total speakers found: {len(total_speakers)}, Confident speakers: {len(confident_speakers)}")
            logger.info(f"   Speaker confidence threshold: {speaker_confidence_threshold}")
            return transcription_result
            
        except Exception as e:
            logger.error(f"Speaker assignment failed: {e}")
            logger.error(traceback.format_exc())
            return transcription_result

    def filter_spurious_speakers(self, result: dict, min_speaker_duration: float = 3.0, speaker_confidence_threshold: float = 0.6) -> dict:
        """
        Remove speakers with less than minimum speaking time using 
        confidence-based filtering and multiple criteria
        """
        try:
            # Calculate total speaking time per speaker AND average confidence
            speaker_stats = {}
            for segment in result['segments']:
                if 'words' not in segment:
                    continue
                for word in segment['words']:
                    speaker = word.get('speaker', 'SPEAKER_00')
                    confidence = word.get('speaker_confidence', 0.5)
                    duration = word.get('end', 0) - word.get('start', 0)
                    
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = {
                            'total_duration': 0,
                            'confidences': [],
                            'word_count': 0
                        }
                    
                    speaker_stats[speaker]['total_duration'] += duration
                    speaker_stats[speaker]['confidences'].append(confidence)
                    speaker_stats[speaker]['word_count'] += 1
            
            # Calculate average confidence per speaker
            for speaker in speaker_stats:
                confidences = speaker_stats[speaker]['confidences']
                speaker_stats[speaker]['avg_confidence'] = sum(confidences) / len(confidences)
            
            # Multi-criteria spurious speaker detection
            spurious_speakers = []
            
            # ADAPTIVE FILTERING: More aggressive for fewer speakers
            total_speakers = len(speaker_stats)
            if total_speakers <= 3:
                # For simple audio (â‰¤3 speakers), be MUCH more aggressive  
                duration_multiplier = 2.0  # Require 2x more duration (6.0s instead of 3.0s)
                confidence_multiplier = 1.2  # Require 20% higher confidence
                word_multiplier = 1.5  # Require 50% more words
            else:
                # For complex audio (>3 speakers), be more lenient
                duration_multiplier = 0.8
                confidence_multiplier = 0.9
                word_multiplier = 0.8
            
            for speaker, stats in speaker_stats.items():
                is_spurious = False
                reasons = []
                
                # Criterion 1: Duration too short (with adaptive threshold)
                adaptive_min_duration = min_speaker_duration * duration_multiplier
                if stats['total_duration'] < adaptive_min_duration:
                    is_spurious = True
                    reasons.append(f"duration {stats['total_duration']:.1f}s < {adaptive_min_duration:.1f}s (adaptive)")
                
                # Criterion 2: Average confidence too low - USE RELATIVE THRESHOLD (with adaptive multiplier)
                # Instead of absolute 0.6/0.7, use relative to other speakers
                all_confidences = [stats['avg_confidence'] for stats in speaker_stats.values()]
                if len(all_confidences) > 1:
                    base_threshold = min(speaker_confidence_threshold, 
                                       max(all_confidences) * 0.75)  # 75% of highest confidence
                    confidence_threshold = base_threshold * confidence_multiplier
                else:
                    confidence_threshold = speaker_confidence_threshold * 0.8 * confidence_multiplier  # More lenient for single speaker
                    
                if stats['avg_confidence'] < confidence_threshold:
                    is_spurious = True
                    reasons.append(f"confidence {stats['avg_confidence']:.3f} < {confidence_threshold:.3f} (adaptive)")
                
                # Criterion 3: Too few words (likely noise) - with adaptive threshold
                adaptive_min_words = max(5, adaptive_min_duration * 2 * word_multiplier)  # Apply word multiplier
                if stats['word_count'] < adaptive_min_words:
                    is_spurious = True
                    reasons.append(f"word count {stats['word_count']} < {adaptive_min_words:.0f} (adaptive)")
                
                if is_spurious:
                    spurious_speakers.append(speaker)
                    logger.info(f"Marking {speaker} as spurious: {', '.join(reasons)}")
            
            if spurious_speakers:
                logger.info(f"Removing {len(spurious_speakers)} spurious speakers: {spurious_speakers}")
                
                # Find the most confident speaker as reassignment target
                if speaker_stats:
                    # Sort by confidence * duration to find most reliable speaker
                    reliable_speakers = {
                        spk: stats['avg_confidence'] * stats['total_duration']
                        for spk, stats in speaker_stats.items()
                        if spk not in spurious_speakers
                    }
                    
                    if reliable_speakers:
                        dominant_speaker = max(reliable_speakers.items(), key=lambda x: x[1])[0]
                        logger.info(f"Reassigning spurious speakers to most reliable speaker: {dominant_speaker}")
                    else:
                        dominant_speaker = "SPEAKER_00"  # Fallback
                        logger.warning("No reliable speakers found, using SPEAKER_00 as fallback")
                
                    # Reassign spurious speaker words and segments
                    for segment in result['segments']:
                        if 'words' not in segment:
                            continue
                        for word in segment['words']:
                            if word.get('speaker') in spurious_speakers:
                                word['speaker'] = dominant_speaker
                                word['speaker_confidence'] = 0.6  # Medium confidence for reassigned
                        
                        # Update segment speaker
                        if segment.get('speaker') in spurious_speakers:
                            segment['speaker'] = dominant_speaker
                            segment['speaker_confidence'] = 0.6
            
            # Final count of remaining speakers
            remaining_speakers = set()
            for segment in result['segments']:
                if 'speaker' in segment and segment.get('speaker') not in spurious_speakers:
                    remaining_speakers.add(segment['speaker'])
            
            logger.info(f"âœ… Spurious speaker filtering complete - {len(remaining_speakers)} speakers remaining")
            return result
            
        except Exception as e:
            logger.error(f"Error filtering spurious speakers: {e}")
            logger.error(traceback.format_exc())
            return result

    def smooth_speaker_changes(self, result: dict, speaker_smoothing_enabled: bool = True, min_switch_duration: float = 2.0) -> dict:
        """
        OPTIMIZATION 1: Speaker Smoothing
        Reduces rapid speaker A â†’ B â†’ A switches that are likely errors
        """
        try:
            if not speaker_smoothing_enabled:
                return result
                
            segments = result.get('segments', [])
            if len(segments) < 3:
                return result  # Need at least 3 segments to smooth
            
            logger.info("Applying speaker smoothing to reduce rapid switches...")
            changes_made = 0
            
            # Look for A â†’ B â†’ A patterns where B is very short
            for i in range(1, len(segments) - 1):
                prev_segment = segments[i - 1]
                current_segment = segments[i]
                next_segment = segments[i + 1]
                
                prev_speaker = prev_segment.get('speaker')
                current_speaker = current_segment.get('speaker') 
                next_speaker = next_segment.get('speaker')
                
                # Check for A â†’ B â†’ A pattern
                if (prev_speaker == next_speaker and 
                    current_speaker != prev_speaker and
                    prev_speaker is not None and current_speaker is not None):
                    
                    # Check if middle segment is too short
                    current_duration = current_segment.get('end', 0) - current_segment.get('start', 0)
                    
                    if current_duration < min_switch_duration:
                        # Check confidence levels
                        prev_conf = prev_segment.get('speaker_confidence', 0.5)
                        current_conf = current_segment.get('speaker_confidence', 0.5)
                        next_conf = next_segment.get('speaker_confidence', 0.5)
                        
                        # If surrounding segments are more confident, smooth the middle one
                        avg_surrounding_conf = (prev_conf + next_conf) / 2
                        if avg_surrounding_conf > current_conf:
                            logger.debug(f"Smoothing speaker switch: {prev_speaker}â†’{current_speaker}â†’{next_speaker} "
                                       f"(duration: {current_duration:.1f}s, conf: {current_conf:.3f})")
                            
                            # Change middle segment to match surrounding
                            segments[i]['speaker'] = prev_speaker
                            segments[i]['speaker_confidence'] = avg_surrounding_conf
                            
                            # Also smooth the words in this segment
                            if 'words' in segments[i]:
                                for word in segments[i]['words']:
                                    word['speaker'] = prev_speaker
                                    word['speaker_confidence'] = avg_surrounding_conf
                            
                            changes_made += 1
            
            if changes_made > 0:
                logger.info(f"âœ… Speaker smoothing complete - processed {changes_made} rapid switches")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in speaker smoothing: {e}")
            return result

    def validate_speakers_with_vad(self, result: dict, audio_path: str = None, cached_waveform_data=None) -> dict:
        """
        OPTIMIZATION 2: VAD Cross-Reference (OPTIMIZED)
        Validates speaker segments against Voice Activity Detection
        Now uses cached waveform data to avoid redundant audio loading
        """
        try:
            logger.info("Validating speakers with VAD...")
            
            # Use cached waveform data if available, otherwise load audio
            if cached_waveform_data is not None:
                waveform, sample_rate = cached_waveform_data
                logger.info("Using cached waveform for VAD validation")
            else:
                # Fallback to loading audio (should rarely happen now)
                logger.info("Loading audio for VAD validation (cache miss)")
                waveform, sample_rate = torchaudio.load(audio_path)
            
            # Ensure mono
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # Simple energy-based VAD
            frame_length = int(0.025 * sample_rate)  # 25ms frames
            hop_length = int(0.010 * sample_rate)    # 10ms hop
            
            # Calculate energy in frames
            energy = torchaudio.functional.compute_deltas(
                waveform.unfold(1, frame_length, hop_length).pow(2).mean(2)
            )
            
            # Simple threshold-based VAD (improve this later)
            energy_threshold = energy.mean() + 2 * energy.std()
            voice_activity = energy > energy_threshold
            
            # Convert frame indices to time
            frame_times = torch.arange(voice_activity.shape[1]) * hop_length / sample_rate
            
            # Validate each segment against VAD
            segments = result.get('segments', [])
            validated_segments = []
            
            for segment in segments:
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                
                # Find overlapping VAD frames
                segment_mask = (frame_times >= start_time) & (frame_times <= end_time)
                if segment_mask.sum() == 0:
                    continue  # Skip if no frames
                
                voice_ratio = voice_activity[0][segment_mask].float().mean().item()
                
                # Only keep segments with significant voice activity
                if voice_ratio > 0.1:  # REDUCED from 0.3 to 0.1 â†’ less aggressive filtering
                    segment['vad_confidence'] = voice_ratio
                    validated_segments.append(segment)
                else:
                    logger.debug(f"Removing segment with low VAD: {voice_ratio:.3f}")
            
            result['segments'] = validated_segments
            logger.info(f"âœ… VAD validation complete - kept {len(validated_segments)}/{len(segments)} segments")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in VAD validation: {e}")
            # Return original result if VAD fails
            return result

    def apply_hierarchical_clustering(self, result: dict) -> dict:
        """
        OPTIMIZATION 3: Hierarchical Clustering Refinement
        Uses multiple clustering approaches to improve speaker separation
        ADAPTIVE: Aggressive merging only when over-detection is likely
        """
        try:
            logger.info("Applying hierarchical clustering refinement...")
            
            segments = result.get('segments', [])
            if len(segments) < 2:
                return result
            
            # Extract speaker embeddings (simulated - in real implementation, 
            # you'd extract actual speaker embeddings)
            speaker_data = {}
            for segment in segments:
                speaker = segment.get('speaker')
                if speaker not in speaker_data:
                    speaker_data[speaker] = {
                        'durations': [],
                        'confidences': [],
                        'positions': []  # Position in audio
                    }
                
                duration = segment.get('end', 0) - segment.get('start', 0)
                confidence = segment.get('speaker_confidence', 0.5)
                position = segment.get('start', 0)
                
                speaker_data[speaker]['durations'].append(duration)
                speaker_data[speaker]['confidences'].append(confidence)
                speaker_data[speaker]['positions'].append(position)
            
            initial_speaker_count = len(speaker_data)
            
            # ADAPTIVE MERGING: Only be aggressive when we suspect over-detection
            if initial_speaker_count == 2:
                # For 2 speakers, don't merge (likely correct)
                logger.info(f"2 speakers detected - skipping hierarchical merging")
                return result
            elif initial_speaker_count == 3:
                # For 3 speakers, be VERY aggressive (likely over-detection in simple audio)
                similarity_threshold = 0.6  # Between 0.508 (no) and 0.636 (yes)
                duration_threshold = 200  # Very high threshold - merge any speaker with <200s
                logger.info(f"3 speakers detected - VERY aggressive merging (similarity: {similarity_threshold})")
            else:
                # For >3 speakers, be conservative (likely legitimate complex audio)
                # BUT use duration-based adaptive merging
                if initial_speaker_count <= 5:
                    # For 4-5 speakers, be VERY conservative (avoid breaking known good results)
                    similarity_threshold = 0.85  # RESTORED original threshold
                    duration_threshold = 10      # Only merge very short speakers
                    logger.info(f"{initial_speaker_count} speakers detected - VERY conservative merging (similarity: {similarity_threshold})")
                elif initial_speaker_count <= 7:
                    # For 6-7 speakers, be moderately aggressive
                    similarity_threshold = 0.8   # Allow some merging
                    duration_threshold = 30      # Merge longer speakers
                    logger.info(f"{initial_speaker_count} speakers detected - moderately conservative merging (similarity: {similarity_threshold})")
                else:
                    # For 8+ speakers, be MORE aggressive (likely significant over-detection)  
                    similarity_threshold = 0.75  # Lower threshold to allow more merging
                    duration_threshold = 40      # Merge even longer speakers
                    logger.info(f"{initial_speaker_count} speakers detected - aggressive merging (similarity: {similarity_threshold})")
            
            # Calculate speaker similarity metrics with adaptive thresholds
            speakers = list(speaker_data.keys())
            merge_made = False  # Prevent cascading merges
            
            for i, spk1 in enumerate(speakers):
                if merge_made:  # Only allow one merge per iteration
                    break
                for j, spk2 in enumerate(speakers[i+1:], i+1):
                    # Simple similarity based on timing and confidence patterns
                    avg_conf1 = sum(speaker_data[spk1]['confidences']) / len(speaker_data[spk1]['confidences'])
                    avg_conf2 = sum(speaker_data[spk2]['confidences']) / len(speaker_data[spk2]['confidences'])
                    
                    total_dur1 = sum(speaker_data[spk1]['durations'])
                    total_dur2 = sum(speaker_data[spk2]['durations'])
                    
                    # If speakers are very similar in confidence and have short durations,
                    # they might be the same person
                    conf_similarity = 1 - abs(avg_conf1 - avg_conf2)
                    dur_ratio = min(total_dur1, total_dur2) / max(total_dur1, total_dur2)
                    
                    similarity = (conf_similarity + dur_ratio) / 2
                    
                    # Debug logging for 3-speaker cases
                    if initial_speaker_count == 3:
                        logger.info(f"Speaker pair {spk1}-{spk2}: similarity={similarity:.3f}, dur1={total_dur1:.1f}s, dur2={total_dur2:.1f}s, conf1={avg_conf1:.3f}, conf2={avg_conf2:.3f}")
                    
                    # Apply adaptive thresholds
                    if similarity > similarity_threshold and (total_dur1 < duration_threshold or total_dur2 < duration_threshold):
                        logger.info(f"Merging speakers: {spk1} â†” {spk2} (similarity: {similarity:.3f}, dur1: {total_dur1:.1f}s, dur2: {total_dur2:.1f}s)")
                        
                        # Merge the less confident speaker into the more confident one
                        if avg_conf1 > avg_conf2:
                            # Merge spk2 â†’ spk1
                            for segment in segments:
                                if segment.get('speaker') == spk2:
                                    segment['speaker'] = spk1
                                    # Update words too
                                    if 'words' in segment:
                                        for word in segment['words']:
                                            if word.get('speaker') == spk2:
                                                word['speaker'] = spk1
                        else:
                            # Merge spk1 â†’ spk2  
                            for segment in segments:
                                if segment.get('speaker') == spk1:
                                    segment['speaker'] = spk2
                                    if 'words' in segment:
                                        for word in segment['words']:
                                            if word.get('speaker') == spk1:
                                                word['speaker'] = spk2
                        merge_made = True  # Prevent additional merges
                        break  # Exit inner loop
            
            # Recount speakers after potential merging
            final_speakers = set()
            for segment in segments:
                if 'speaker' in segment:
                    final_speakers.add(segment['speaker'])
            
            logger.info(f"âœ… Hierarchical clustering complete - {len(final_speakers)} final speakers (was {initial_speaker_count})")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in hierarchical clustering: {e}")
            return result

    def preprocess_audio_for_diarization(self, audio_path: str) -> tuple[str, tuple]:
        """
        OPTIMIZED: Preprocess audio and cache waveform to prevent redundant loading
        Returns: (temp_path, (waveform, sample_rate))
        """
        try:
            # Check if we already have cached data for this audio path
            if self.cached_audio_path == audio_path and self.cached_preprocessed_audio is not None:
                logger.info(f"Using cached preprocessed audio for: {audio_path}")
                # Still need to create temp file for diarization pipeline
                waveform, sample_rate = self.cached_preprocessed_audio
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                    temp_path = tmp_file.name
                    torchaudio.save(temp_path, waveform, sample_rate)
                return temp_path, (waveform, sample_rate)
            
            # Load and preprocess audio
            logger.info(f"Preprocessing audio for diarization: {audio_path}")
            waveform, sample_rate = torchaudio.load(audio_path)
            
            # Ensure mono audio
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # Resample to 16kHz if needed
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # Cache the preprocessed audio
            self.cached_preprocessed_audio = (waveform, sample_rate)
            self.cached_audio_path = audio_path
            
            # Create temporary file for diarization
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                temp_path = tmp_file.name
                torchaudio.save(temp_path, waveform, sample_rate)
            
            logger.info(f"Audio preprocessed and cached: {audio_path} -> {temp_path}")
            return temp_path, (waveform, sample_rate)
            
        except Exception as e:
            logger.error(f"Audio preprocessing failed: {e}")
            # Return original path and None for waveform data if preprocessing fails
            return audio_path, None

    async def transcribe_with_diarization(
        self, 
        audio_path: str, 
        enable_diarization: bool = True,
        language: str = "en",
        model: str = "large-v3-turbo", 
        batch_size: int = 16,
        prompt: str = "",
        temperature: float = 0.0,
        timestamp_granularities: str = "segment",
        output_content: str = "both",
        clustering_threshold: float = 0.7,
        segmentation_threshold: float = 0.45,
        min_speaker_duration: float = 3.0,
        speaker_confidence_threshold: float = 0.6,
        speaker_smoothing_enabled: bool = True,
        min_switch_duration: float = 2.0,
        vad_validation_enabled: bool = False,
        optimized_alignment: bool = True
    ) -> dict:
        """
        Advanced transcription with diarization and storage integration
        """
        try:
            # Load models if not already loaded
            if not self.transcription_model:
                self.load_models()
            
            # Step 1: Transcribe
            logger.info("Starting transcription...")
            start_time = time.time()
            
            # OPTIMIZATION: Strategic GPU Memory management
            if self.device == "cuda" and torch.cuda.is_available():
                # Only clear cache at start of request, not between each step
                torch.cuda.empty_cache()  # Clear GPU cache once at start
                initial_memory = torch.cuda.memory_allocated() / 1e6  # MB
                logger.info(f"GPU memory before transcription: {initial_memory:.1f} MB")
            
            # Build transcription options
            transcribe_options = {
                "batch_size": batch_size,
                "language": language
            }
            
            # Add optional parameters if provided
            if prompt and prompt.strip():
                transcribe_options["initial_prompt"] = prompt.strip()
            
            if temperature > 0:
                transcribe_options["temperature"] = temperature
            
            result = self.transcription_model.transcribe(
                audio_path,
                **transcribe_options
            )
            transcribe_time = time.time() - start_time
            logger.info(f"Transcription completed in {transcribe_time:.1f}s")
            
            # OPTIMIZATION: Skip redundant cache clearing between steps
            
            # Step 2: Align (Conditional based on optimized_alignment flag)
            if optimized_alignment and self.alignment_model and self.align_model_metadata:
                logger.info("Using WAV2VEC2 alignment model for optimized alignment...")
                start_time = time.time()
                result = whisperx.align(
                    result["segments"],
                    self.alignment_model,
                    self.align_model_metadata,
                    audio_path,
                    self.device,
                    return_char_alignments=False
                )
                align_time = time.time() - start_time
                logger.info(f"WAV2VEC2 alignment completed in {align_time:.1f}s")
            else:
                logger.info("Using Whisper built-in alignment (faster processing)...")
                # Use Whisper's built-in timestamps - no external alignment needed
                align_time = 0
            
            # Step 3: Diarization
            if enable_diarization and self.diarization_pipeline:
                preprocessed_audio_path = None
                try:
                    logger.info("Starting diarization...")
                    start_time = time.time()
                    
                    # OPTIMIZATION: Skip redundant cache clearing before diarization
                    if self.device == "cuda" and torch.cuda.is_available():
                        initial_memory = torch.cuda.memory_allocated() / 1e6  # MB
                        logger.info(f"GPU memory before diarization: {initial_memory:.1f} MB")
                    
                    # Preprocess audio for diarization
                    preprocessed_audio_path, cached_waveform_data = self.preprocess_audio_for_diarization(audio_path)
                    waveform, sample_rate = cached_waveform_data if cached_waveform_data else (None, None)
                    
                    # Run diarization with GPU optimizations
                    # GPU-optimized parameters for faster processing
                    if hasattr(self.diarization_pipeline, '_segmentation'):
                        # Enable GPU batch processing if available
                        if hasattr(self.diarization_pipeline._segmentation, 'batch_size'):
                            self.diarization_pipeline._segmentation.batch_size = 32
                    
                    if hasattr(self.diarization_pipeline, '_embedding'):
                        if hasattr(self.diarization_pipeline._embedding, 'batch_size'):
                            self.diarization_pipeline._embedding.batch_size = 32
                    
                    # Configure diarization parameters directly on pipeline components
                    # (pyannote.audio 3.3.2 doesn't accept parameters in apply() call)
                    if hasattr(self.diarization_pipeline, '_segmentation') and hasattr(self.diarization_pipeline._segmentation, 'instantiate'):
                        try:
                            self.diarization_pipeline._segmentation.instantiate().threshold = segmentation_threshold
                        except:
                            pass  # If configuration fails, continue with defaults
                    
                    if hasattr(self.diarization_pipeline, '_clustering') and hasattr(self.diarization_pipeline._clustering, 'instantiate'):
                        try:
                            self.diarization_pipeline._clustering.instantiate().threshold = clustering_threshold  
                        except:
                            pass  # If configuration fails, continue with defaults
                    
                    # Run diarization on GPU (no parameters - configured above)
                    diarization_result = self.diarization_pipeline(preprocessed_audio_path)
                    
                    # OPTIMIZATION: Use combined post-processing pipeline
                    result = self.optimized_diarization_postprocessing(
                        result, diarization_result, speaker_confidence_threshold,
                        min_speaker_duration, speaker_smoothing_enabled, min_switch_duration,
                        vad_validation_enabled, audio_path, cached_waveform_data
                    )
                    
                    diarize_time = time.time() - start_time
                    
                    # OPTIMIZATION: Final GPU cleanup and performance logging
                    if self.device == "cuda" and torch.cuda.is_available():
                        final_memory = torch.cuda.memory_allocated() / 1e6  # MB
                        # Only clean up GPU memory once at the very end
                        torch.cuda.empty_cache()
                        logger.info(f"GPU memory after diarization: {final_memory:.1f} MB")
                        logger.info(f"ðŸš€ GPU-accelerated diarization completed in {diarize_time:.1f}s")
                    else:
                        logger.info(f"Diarization completed in {diarize_time:.1f}s")
                    
                except Exception as e:
                    logger.error(f"Diarization failed but continuing: {e}")
                    # Continue without diarization rather than crash
                finally:
                    # Clean up temporary preprocessed audio file
                    if preprocessed_audio_path and preprocessed_audio_path != audio_path and os.path.exists(preprocessed_audio_path):
                        try:
                            os.unlink(preprocessed_audio_path)
                            logger.debug(f"Cleaned up temporary audio file: {preprocessed_audio_path}")
                        except Exception as cleanup_error:
                            logger.warning(f"Failed to clean up temporary file {preprocessed_audio_path}: {cleanup_error}")
            
            # Count speakers from segments
            total_speakers = 0
            confident_speakers = 0
            if 'segments' in result:
                speakers_found = set()
                confident_speakers_found = set()
                for segment in result['segments']:
                    if 'speaker' in segment:
                        speakers_found.add(segment['speaker'])
                        if segment.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                            confident_speakers_found.add(segment['speaker'])
                
                total_speakers = len(speakers_found)
                confident_speakers = len(confident_speakers_found)
            
            # Apply timestamp granularity filtering
            granularities = [g.strip().lower() for g in timestamp_granularities.split(',')]
            
            # Convert numpy types to JSON-serializable types first
            result = convert_numpy_types(result)
            
            # Create the base response structure
            filtered_result = {}
            
            # Handle timestamp granularities filtering with improved logic
            logger.info(f"Applying timestamp granularities filter: {granularities}")
            
            if 'segment' in granularities and 'word' in granularities:
                # Both segment and word level - keep everything
                filtered_result = result.copy()
                logger.info("Keeping both segment and word level timestamps")
            elif 'segment' in granularities and 'word' not in granularities:
                # Only segment level - remove ALL word-level data
                filtered_result = result.copy()
                logger.info("Filtering to segment-only timestamps - removing word data")
                
                # Remove root level word arrays
                filtered_result.pop('words', None)
                filtered_result.pop('word_segments', None)  # In case this exists
                
                # Remove word-level data from segments
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        # Remove word-level timestamps but keep segment-level speaker info
                        segment.pop('words', None)
                        # Also remove any other word-level fields that might exist
                        segment.pop('word_segments', None)
                        
                logger.info(f"Segments after word removal: {len(filtered_result.get('segments', []))}")
                
            elif 'word' in granularities and 'segment' not in granularities:
                # Only word level - remove segments completely, keep only word data
                filtered_result = result.copy()
                logger.info("Filtering to word-only timestamps - removing segments")
                
                # Collect all words from segments before removing segments
                all_words = []
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        if 'words' in segment:
                            all_words.extend(segment['words'])
                
                # If no root-level words exist but we have segment words, move them to root
                if 'words' not in filtered_result and all_words:
                    filtered_result['words'] = all_words
                
                # Completely remove segments structure
                filtered_result.pop('segments', None)
                
                # Keep only word-level data at root level
                logger.info(f"Word-only result structure: root_words={'words' in filtered_result}, total_words={len(filtered_result.get('words', []))}, segments_removed=True")
                        
            else:
                # Default: keep segments if available, remove words to match default behavior
                filtered_result = result.copy()
                logger.info("Using default granularity (segment-only)")
                # Default to segment-only behavior
                filtered_result.pop('words', None)
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        segment.pop('words', None)
            
            # Handle diarization consistency
            if not enable_diarization:
                # Remove all speaker-related information
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        segment.pop('speaker', None)
                        segment.pop('speaker_confidence', None)
                        if 'words' in segment:
                            for word in segment['words']:
                                word.pop('speaker', None)
                                word.pop('speaker_confidence', None)
                if 'words' in filtered_result:
                    for word in filtered_result['words']:
                        word.pop('speaker', None)
                        word.pop('speaker_confidence', None)
            
            # Build processing info
            processing_info = {
                "device": self.device,
                "language_detected": filtered_result.get("language", "en"),
                "audio_duration": getattr(filtered_result, "duration", 0),
                "total_speakers": total_speakers,
                "confident_speakers": confident_speakers,
                "timestamp_granularities": granularities,
                "diarization_enabled": enable_diarization
            }
            
            # Generate text-only content for use in various output formats
            text_content = ""
            if 'segments' in filtered_result:
                text_content = "\n".join([segment.get('text', '') for segment in filtered_result.get('segments', [])])
            elif 'words' in filtered_result:
                # Extract text from word-level data when segments are not available
                words_text = ' '.join([word.get('word', word.get('text', '')) for word in filtered_result.get('words', [])])
                text_content = words_text
            elif 'text' in filtered_result:
                text_content = filtered_result['text']
            
            # Filter response based on output_content
            if output_content == "text_only":
                # Return only the transcribed text, no timestamps
                return {"text": text_content.strip()}
            
            elif output_content == "timestamps_only":
                # Return timestamped segments with text (respecting granularity settings)
                logger.info(f"Processing timestamps_only output with granularities: {granularities}")
                
                # Return the filtered result with timestamps AND text content
                return {
                    "status": "success", 
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
            elif output_content == "both":
                # Return separate text-only section AND timestamped results
                return {
                    "status": "success",
                    "text": text_content.strip(),
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
            elif output_content == "metadata_only":
                # Return only processing info and metadata
                speakers_list = []
                if enable_diarization and 'segments' in filtered_result:
                    speakers_list = list(set(segment.get('speaker', 'SPEAKER_00') 
                                           for segment in filtered_result.get('segments', []) 
                                           if 'speaker' in segment))
                
                return {
                    "status": "success",
                    "processing_info": processing_info,
                    "speakers": speakers_list
                }
            
            else:
                # Default fallback to 'both' format
                return {
                    "status": "success",
                    "text": text_content.strip(),
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")
        finally:
            # OPTIMIZATION: Clear audio cache to prevent memory leaks
            self.clear_audio_cache()

    def optimized_diarization_postprocessing(self, result: dict, diarization_result, speaker_confidence_threshold: float, min_speaker_duration: float, speaker_smoothing_enabled: bool, min_switch_duration: float, vad_validation_enabled: bool, audio_path: str = None, cached_waveform_data=None) -> dict:
        """
        OPTIMIZATION: Combined post-processing pipeline for better performance
        Combines speaker assignment, filtering, and smoothing in optimized sequence
        """
        try:
            logger.info("Starting optimized diarization post-processing pipeline...")
            
            # Step 1: Manual speaker assignment with confidence scoring
            result = self.manual_speaker_assignment(result, diarization_result, speaker_confidence_threshold)
            
            # Step 2: Combined spurious speaker filtering and smoothing
            # (More efficient than separate passes)
            result = self.combined_speaker_filtering_and_smoothing(
                result, min_speaker_duration, speaker_confidence_threshold,
                speaker_smoothing_enabled, min_switch_duration
            )
            
            # Step 3: VAD validation (only if enabled and we have cached data)
            if vad_validation_enabled:
                result = self.validate_speakers_with_vad(result, audio_path, cached_waveform_data)
            
            # Step 4: Hierarchical clustering refinement
            result = self.apply_hierarchical_clustering(result)
            
            logger.info("âœ… Optimized post-processing pipeline complete")
            return result
            
        except Exception as e:
            logger.error(f"Error in optimized post-processing: {e}")
            return result

    def combined_speaker_filtering_and_smoothing(self, result: dict, min_speaker_duration: float, speaker_confidence_threshold: float, speaker_smoothing_enabled: bool, min_switch_duration: float) -> dict:
        """
        OPTIMIZATION: Combined filtering and smoothing in single pass
        More efficient than separate operations
        """
        try:
            segments = result.get('segments', [])
            if len(segments) < 2:
                return result
                
            logger.info("Applying combined speaker filtering and smoothing...")
            
            # First pass: Calculate speaker statistics for filtering
            # Handle both word-level and segment-level modes
            speaker_stats = {}
            for segment in segments:
                if 'words' in segment:
                    # Word-level mode
                    for word in segment['words']:
                        speaker = word.get('speaker', 'SPEAKER_00')
                        confidence = word.get('speaker_confidence', 0.5)
                        duration = word.get('end', 0) - word.get('start', 0)
                        
                        if speaker not in speaker_stats:
                            speaker_stats[speaker] = {
                                'total_duration': 0,
                                'confidences': [],
                                'word_count': 0
                            }
                        
                        speaker_stats[speaker]['total_duration'] += duration
                        speaker_stats[speaker]['confidences'].append(confidence)
                        speaker_stats[speaker]['word_count'] += 1
                else:
                    # Segment-level mode
                    speaker = segment.get('speaker', 'SPEAKER_00')
                    confidence = segment.get('speaker_confidence', 0.5)
                    duration = segment.get('end', 0) - segment.get('start', 0)
                    
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = {
                            'total_duration': 0,
                            'confidences': [],
                            'word_count': 0
                        }
                    
                    speaker_stats[speaker]['total_duration'] += duration
                    speaker_stats[speaker]['confidences'].append(confidence)
                    speaker_stats[speaker]['word_count'] += 1
            
            # Calculate average confidence per speaker
            for speaker in speaker_stats:
                confidences = speaker_stats[speaker]['confidences']
                speaker_stats[speaker]['avg_confidence'] = sum(confidences) / len(confidences)
            
            # Identify spurious speakers with adaptive filtering
            spurious_speakers = self.identify_spurious_speakers(speaker_stats, min_speaker_duration, speaker_confidence_threshold)
            
            # Find dominant speaker for reassignment
            dominant_speaker = self.find_dominant_speaker(speaker_stats, spurious_speakers)
            
            # Combined pass: Apply smoothing AND spurious speaker reassignment
            changes_made = 0
            reassignments_made = 0
            
            for i, segment in enumerate(segments):
                # Spurious speaker reassignment
                if segment.get('speaker') in spurious_speakers:
                    segment['speaker'] = dominant_speaker
                    segment['speaker_confidence'] = 0.6
                    if 'words' in segment:
                        for word in segment['words']:
                            if word.get('speaker') in spurious_speakers:
                                word['speaker'] = dominant_speaker
                                word['speaker_confidence'] = 0.6
                    reassignments_made += 1
                
                # Speaker smoothing (A â†’ B â†’ A pattern detection)
                if (speaker_smoothing_enabled and i > 0 and i < len(segments) - 1):
                    prev_segment = segments[i - 1]
                    next_segment = segments[i + 1]
                    
                    prev_speaker = prev_segment.get('speaker')
                    current_speaker = segment.get('speaker')
                    next_speaker = next_segment.get('speaker')
                    
                    # Check for A â†’ B â†’ A pattern
                    if (prev_speaker == next_speaker and 
                        current_speaker != prev_speaker and
                        prev_speaker is not None and current_speaker is not None):
                        
                        current_duration = segment.get('end', 0) - segment.get('start', 0)
                        
                        if current_duration < min_switch_duration:
                            # Check confidence levels
                            prev_conf = prev_segment.get('speaker_confidence', 0.5)
                            current_conf = segment.get('speaker_confidence', 0.5)
                            next_conf = next_segment.get('speaker_confidence', 0.5)
                            
                            avg_surrounding_conf = (prev_conf + next_conf) / 2
                            if avg_surrounding_conf > current_conf:
                                # Apply smoothing
                                segment['speaker'] = prev_speaker
                                segment['speaker_confidence'] = avg_surrounding_conf
                                
                                if 'words' in segment:
                                    for word in segment['words']:
                                        word['speaker'] = prev_speaker
                                        word['speaker_confidence'] = avg_surrounding_conf
                                
                                changes_made += 1
            
            if spurious_speakers:
                logger.info(f"âœ… Removed {len(spurious_speakers)} spurious speakers, reassigned {reassignments_made} segments")
            
            if changes_made > 0:
                logger.info(f"âœ… Applied speaker smoothing to {changes_made} segments")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in combined filtering and smoothing: {e}")
            return result

    def identify_spurious_speakers(self, speaker_stats: dict, min_speaker_duration: float, speaker_confidence_threshold: float) -> list:
        """Helper method to identify spurious speakers with adaptive thresholds"""
        spurious_speakers = []
        total_speakers = len(speaker_stats)
        
        # Adaptive filtering based on speaker count
        if total_speakers <= 3:
            duration_multiplier = 2.0
            confidence_multiplier = 1.2
            word_multiplier = 1.5
        else:
            duration_multiplier = 0.8
            confidence_multiplier = 0.9
            word_multiplier = 0.8
        
        for speaker, stats in speaker_stats.items():
            is_spurious = False
            
            # Duration check
            adaptive_min_duration = min_speaker_duration * duration_multiplier
            if stats['total_duration'] < adaptive_min_duration:
                is_spurious = True
            
            # Confidence check
            all_confidences = [stats['avg_confidence'] for stats in speaker_stats.values()]
            if len(all_confidences) > 1:
                base_threshold = min(speaker_confidence_threshold, max(all_confidences) * 0.75)
                confidence_threshold = base_threshold * confidence_multiplier
            else:
                confidence_threshold = speaker_confidence_threshold * 0.8 * confidence_multiplier
            
            if stats['avg_confidence'] < confidence_threshold:
                is_spurious = True
            
            # Word count check
            adaptive_min_words = max(5, adaptive_min_duration * 2 * word_multiplier)
            if stats['word_count'] < adaptive_min_words:
                is_spurious = True
            
            if is_spurious:
                spurious_speakers.append(speaker)
        
        return spurious_speakers

    def find_dominant_speaker(self, speaker_stats: dict, spurious_speakers: list) -> str:
        """Helper method to find the most reliable speaker for reassignment"""
        if not speaker_stats:
            return "SPEAKER_00"
        
        reliable_speakers = {
            spk: stats['avg_confidence'] * stats['total_duration']
            for spk, stats in speaker_stats.items()
            if spk not in spurious_speakers
        }
        
        if reliable_speakers:
            return max(reliable_speakers.items(), key=lambda x: x[1])[0]
        else:
            return "SPEAKER_00"

    def clear_audio_cache(self):
        """
        OPTIMIZATION: Clear cached audio data to prevent memory leaks
        Should be called at the end of each request
        """
        self.cached_preprocessed_audio = None
        self.cached_audio_path = None
        logger.debug("Cleared audio preprocessing cache")

# FastAPI app
app = FastAPI(title="WhisperX Diarization Server with Multi-Cloud Storage")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global server instance
server = WhisperXDiarizationServer()

# Storage utility functions
async def download_from_r2(bucket: str, key: str) -> str:
    """Download file from R2 to temporary file"""
    if not r2_client:
        raise HTTPException(500, "R2 client not configured")
    
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.audio') as temp_file:
            temp_file_path = temp_file.name
            
        r2_client.download_file(bucket, key, temp_file_path)
        logger.info(f"Downloaded R2 object: s3://{bucket}/{key} -> {temp_file_path}")
        return temp_file_path
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'NoSuchKey':
            raise HTTPException(404, f"File not found in R2: {bucket}/{key}")
        elif error_code == 'NoSuchBucket':
            raise HTTPException(404, f"Bucket not found in R2: {bucket}")
        else:
            raise HTTPException(500, f"R2 download failed: {str(e)}")
    except Exception as e:
        raise HTTPException(500, f"Failed to download from R2: {str(e)}")

async def download_from_s3_presigned(presigned_url: str) -> str:
    """Download file from pre-signed S3 URL to temporary file"""
    try:
        response = requests.get(presigned_url, stream=True)
        response.raise_for_status()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.audio') as temp_file:
            temp_file_path = temp_file.name
            
        # Stream download to avoid memory issues with large files
        with open(temp_file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    
        logger.info(f"Downloaded from pre-signed S3 URL -> {temp_file_path}")
        return temp_file_path
        
    except requests.exceptions.RequestException as e:
        raise HTTPException(400, f"Failed to download from pre-signed URL: {str(e)}")
    except Exception as e:
        raise HTTPException(500, f"Unexpected error downloading from S3: {str(e)}")

async def upload_result_to_storage(result: dict, bucket: str, key: str, use_r2: bool = False) -> str:
    """Upload transcription result to R2 or S3 bucket"""
    client = r2_client if use_r2 else s3_client
    
    if not client:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"{service} client not configured")
    
    try:
        # Convert result to JSON and upload
        result_json = json.dumps(result, indent=2, ensure_ascii=False)
        
        client.put_object(
            Bucket=bucket,
            Key=key,
            Body=result_json.encode('utf-8'),
            ContentType='application/json',
            Metadata={
                'uploaded_by': 'whisper-server',
                'timestamp': datetime.now().isoformat(),
                'content_type': 'transcription_result'
            }
        )
        
        service = "R2" if use_r2 else "S3"
        storage_url = f"s3://{bucket}/{key}"
        logger.info(f"Uploaded result to {service}: {storage_url}")
        return storage_url
        
    except ClientError as e:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"Failed to upload result to {service}: {str(e)}")
    except Exception as e:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"Unexpected error uploading to {service}: {str(e)}")

async def upload_to_user_presigned_url(result: dict, presigned_url: str) -> str:
    """Upload transcription result to user-provided pre-signed URL"""
    import httpx
    
    try:
        # Convert result to JSON
        result_json = json.dumps(result, indent=2, ensure_ascii=False)
        
        # Upload using the pre-signed URL
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.put(
                presigned_url,
                content=result_json.encode('utf-8'),
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code not in [200, 201]:
                raise HTTPException(500, f"Failed to upload to user URL: HTTP {response.status_code}")
        
        logger.info(f"Successfully uploaded result to user-provided URL")
        return presigned_url
        
    except httpx.RequestError as e:
        raise HTTPException(500, f"Network error uploading to user URL: {str(e)}")
    except Exception as e:
        raise HTTPException(500, f"Failed to upload to user-provided URL: {str(e)}")

async def process_transcription_background(
    job_id: str,
    temp_audio_path: str,
    upload_presigned_url: str,
    enable_diarization: bool,
    language: str,
    batch_size: int,
    prompt: str,
    timestamp_granularities: str,
    output_content: str,
    clustering_threshold: float,
    segmentation_threshold: float,
    min_speaker_duration: float,
    speaker_confidence_threshold: float,
    speaker_smoothing_enabled: bool,
    min_switch_duration: float,
    vad_validation_enabled: bool,
    optimized_alignment: bool
):
    """
    Background task to process transcription and upload result to user's S3 bucket
    """
    try:
        logger.info(f"ðŸ”„ Starting background transcription for job {job_id}")
        
        # Process transcription
        result = await server.transcribe_with_diarization(
            temp_audio_path, 
            enable_diarization=enable_diarization,
            language=language,
            model="large-v3-turbo",
            batch_size=batch_size,
            prompt=prompt,
            temperature=0.0,
            timestamp_granularities=timestamp_granularities,
            output_content=output_content,
            clustering_threshold=clustering_threshold,
            segmentation_threshold=segmentation_threshold,
            min_speaker_duration=min_speaker_duration,
            speaker_confidence_threshold=speaker_confidence_threshold,
            speaker_smoothing_enabled=speaker_smoothing_enabled,
            min_switch_duration=min_switch_duration,
            vad_validation_enabled=vad_validation_enabled,
            optimized_alignment=optimized_alignment
        )
        
        # Add job ID to result
        if isinstance(result, dict):
            result["job_id"] = job_id
            result["status"] = "completed"
        
        # Upload to user's presigned URL
        await upload_to_user_presigned_url(result, upload_presigned_url)
        
        logger.info(f"âœ… Background job {job_id} completed successfully")
        
    except Exception as e:
        logger.error(f"âŒ Background job {job_id} failed: {e}")
        logger.error(traceback.format_exc())
        
        # Try to upload error result to user's URL
        try:
            error_result = {
                "job_id": job_id,
                "status": "failed",
                "error": str(e),
                "error_type": type(e).__name__
            }
            await upload_to_user_presigned_url(error_result, upload_presigned_url)
        except Exception as upload_error:
            logger.error(f"Failed to upload error result for job {job_id}: {upload_error}")
    
    finally:
        # Cleanup temporary files
        if temp_audio_path and os.path.exists(temp_audio_path):
            try:
                os.unlink(temp_audio_path)
                logger.debug(f"Cleaned up temporary audio file for job {job_id}: {temp_audio_path}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup temporary file {temp_audio_path}: {cleanup_error}")

@app.on_event("startup")
async def startup_event():
    """Initialize models and storage clients on startup"""
    try:
        init_storage_clients()
        server.load_models()
    except Exception as e:
        logger.error(f"Failed to load models on startup: {e}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "device": server.device,
        "models_loaded": {
            "transcription": server.transcription_model is not None,
            "alignment": server.alignment_model is not None,
            "diarization": server.diarization_pipeline is not None
        },
        "storage_clients": {
            "r2_enabled": r2_client is not None,
            "s3_enabled": s3_client is not None
        },
        "features": [
            "Advanced speaker diarization with confidence scoring",
            "Multi-cloud storage support (R2, S3)",
            "GPU-accelerated processing pipeline",
            "Automatic result export to cloud storage",
            "Pre-signed S3 URL support",
            "Direct R2 upload functionality",
            "Optimized clustering and segmentation",
            "Multi-criteria speaker validation",
            "GPU batch processing optimization",
            "Comprehensive error handling and cleanup"
        ]
    }

@app.post("/v1/uploads")
async def upload_files(
    file: UploadFile = File(..., description="Audio file to upload"),
    key: Optional[str] = Form(None, description="Object key (auto-generated if not provided)")
):
    """
    Upload Files
    
    Upload audio files directly to cloud storage and receive the object reference 
    that can be used later for transcription.
    
    **Parameters:**
    - `file`: The audio file to upload  
    - `key`: Optional custom object key (auto-generated if not provided)
    
    **Returns:**
    - Storage object details including bucket, key, and file info
    """
    if not r2_client:
        raise HTTPException(500, "Cloud storage not configured. Check server storage credentials.")
    
    # Use environment-configured bucket
    bucket = DEFAULT_UPLOAD_BUCKET
    logger.info(f"Using configured upload bucket: {bucket}")
        
    # Generate key if not provided
    if not key:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        file_ext = os.path.splitext(file.filename or "audio")[1] or ".wav"
        key = f"uploads/{timestamp}{file_ext}"
    
    try:
        # Read file content
        file_content = await file.read()
        file_size = len(file_content)
        
        # Upload to storage
        r2_client.put_object(
            Bucket=bucket,
            Key=key,
            Body=file_content,
            ContentType=file.content_type or 'audio/wav',
            Metadata={
                'original_filename': file.filename or 'unknown',
                'upload_timestamp': datetime.now().isoformat(),
                'uploaded_by': 'whisper-server'
            }
        )
        
        logger.info(f"Successfully uploaded file to storage: {bucket}/{key} ({file_size} bytes)")
        
        return {
            "status": "success",
            "message": "File uploaded successfully",
            "bucket": bucket,
            "key": key,
            "file_size": file_size,
            "original_filename": file.filename,
            "content_type": file.content_type,
            "upload_timestamp": datetime.now().isoformat(),
            "storage_url": f"s3://{bucket}/{key}"
        }
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'NoSuchBucket':
            raise HTTPException(404, f"Storage bucket not found: {bucket}")
        elif error_code == 'AccessDenied':
            raise HTTPException(403, f"Access denied to storage bucket: {bucket}")
        else:
            raise HTTPException(500, f"Storage upload failed: {str(e)}")
    except Exception as e:
        logger.error(f"Upload to storage failed: {e}")
        raise HTTPException(500, f"Failed to upload file: {str(e)}")

@app.post("/v1/downloads")
async def download_from_storage(
    key: str = Form(..., description="Object key/path of the file to download (e.g., transcriptions/filename.json)")
):
    """
    Download From Storage
    
    Download transcription results and other files from cloud storage.
    
    **Parameters:**
    - `key`: Object key/path of the file to download
    
    **Returns:**
    - The requested file as a downloadable response
    """
    if not r2_client:
        raise HTTPException(500, "Cloud storage client not configured")
    
    # Use environment-configured bucket
    bucket = DEFAULT_RESULTS_BUCKET
    logger.info(f"Downloading from configured results bucket: {bucket}")
    
    try:
        # Get the object from storage
        response = r2_client.get_object(Bucket=bucket, Key=key)
        
        # Get file content and metadata
        file_content = response['Body'].read()
        content_type = response.get('ContentType', 'application/octet-stream')
        
        # Extract filename from key
        filename = key.split('/')[-1] if '/' in key else key
        
        return Response(
            content=file_content,
            media_type=content_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
        
    except Exception as e:
        logger.error(f"Failed to download file {bucket}/{key}: {e}")
        raise HTTPException(404, f"File not found: {str(e)}")

@app.post("/v1/audio/transcriptions")
async def create_transcription(
    request: Request,
    background_tasks: BackgroundTasks,
    
    # === FILE INPUT SECTION ===
    file: Optional[UploadFile] = File(None, description="Audio file to upload and transcribe"),
    
    # === FILE REFERENCE OPTIONS ===
    storage_key: Optional[str] = Form(None, description="Storage object key for existing file"),
    s3_presigned_url: Optional[str] = Form(None, description="Pre-signed URL to download audio file from S3"),
    
    # === TRANSCRIPTION OUTPUT SECTION ===
    response_format: str = Form("json", description="Response format: json, verbose_json, text, srt, vtt"),
    output_content: str = Form("both", description="Controls response content - 'text_only': plain text, 'timestamps_only': JSON with timestamps, 'both': text + timestamps, 'metadata_only': processing info only"),
    stored_output: bool = Form(False, description="Enable result storage - True: Save to storage, False: API response only"),
    output_key: Optional[str] = Form(None, description="Custom storage object key (auto-generated if not provided)"),
    upload_presigned_url: Optional[str] = Form(None, description="Pre-signed upload URL to save results to your own S3 bucket (enables async processing - returns immediately with job status)"),
    
    # === TRANSCRIPTION SETTINGS SECTION ===
    language: str = Form("en", description="Language code (e.g., 'en', 'es', 'fr', 'de')"),
    enable_diarization: bool = Form(True, description="Enable speaker diarization"),
    timestamp_granularities: str = Form("segment", description="Timestamp granularity: 'segment' or 'word' (comma-separated for both)"),
    prompt: str = Form("", description="Optional text prompt to provide context or guide the transcription (e.g., proper names, technical terms)"),
    
    # === FINE-TUNING SETTINGS SECTION ===
    batch_size: int = Form(16, description="Batch size for processing (higher = faster but more memory)"),
    clustering_threshold: float = Form(0.7, description="Speaker clustering threshold (0.5-1.0, lower = more speakers)"),
    segmentation_threshold: float = Form(0.45, description="Voice activity detection threshold (0.1-0.9)"),
    min_speaker_duration: float = Form(3.0, description="Minimum speaking time per speaker (seconds)"),
    speaker_confidence_threshold: float = Form(0.6, description="Minimum confidence for speaker assignment (0.1-1.0)"),
    speaker_smoothing_enabled: bool = Form(True, description="Enable speaker transition smoothing"),
    min_switch_duration: float = Form(2.0, description="Minimum time between speaker switches (seconds)"),
    vad_validation_enabled: bool = Form(False, description="Enable Voice Activity Detection validation (experimental)"),
    optimized_alignment: bool = Form(True, description="Use WAV2VEC2 alignment model (True) or Whisper built-in alignment (False) - False is faster")
):
    """
    **Advanced Audio Transcription with Speaker Identification**
    
    Upload audio files and receive accurate transcriptions with speaker diarization,
    supporting multiple input sources and automatic result export.
    
    **ðŸ“ File Input Options (choose ONE):**
    - `file`: Direct file upload (optional - recommended for most use cases)
    - `storage_key`: Reference to existing file in configured storage bucket  
    - `s3_presigned_url`: Pre-signed URL for secure file access
    
    **Note**: Exactly one input source must be provided. The API will fail if none are provided or if multiple are provided.
    
    **ðŸ“„ Transcription Output:**
    - `response_format`: Choose output format (JSON, SRT, VTT, plain text)
    - `output_content`: Control what's included in response:
      - `text_only`: Returns only plain text transcription (no timestamps)
      - `timestamps_only`: Returns JSON with timestamps and segments (no separate text field)
      - `both`: Returns both plain text AND timestamped segments (default)
      - `metadata_only`: Returns only processing metadata (no transcription text)
    - `stored_output`: Enable/disable result storage (false = API response only)
    - `upload_presigned_url`: Upload to your own S3 bucket (enables async mode)
    - `output_key`: Custom storage object key (auto-generated if not provided)
    
    **âš¡ Processing Modes:**
    - **Async Mode** (when `upload_presigned_url` is provided):
      - Returns immediately with job status and metadata
      - Processes transcription in background
      - Uploads result to your S3 bucket when complete
      - Use S3 event notifications (Lambda) to detect completion
    - **Sync Mode** (when `upload_presigned_url` is NOT provided):
      - Waits for transcription to complete
      - Returns full result in API response
      - Optionally stores result if `stored_output` is true
    
    **Output Logic (Sync Mode):**
    - If `stored_output` is `false`: Results returned in API response only
    - If `stored_output` is `true`:
      - Upload to server's downloads bucket (retrievable via /v1/downloads endpoint)
    
    **âš™ï¸ Transcription Settings:**
    - Multi-language support with automatic detection
    - Advanced speaker diarization with confidence scoring
    - Flexible timestamp granularity (segment, word, or both)
    - Context prompts for improved accuracy with technical terms
    
    **ðŸŽ›ï¸ Fine-Tuning Settings:**
    - Speaker recognition parameters for optimal accuracy
    - GPU batch processing optimization
    - Voice activity detection controls
    - Speaker transition smoothing options
    
    **ðŸš€ Performance Features:**
    - GPU-accelerated processing pipeline
    - Automatic result archiving to cloud storage
    - Enterprise-grade reliability and cleanup
    - Memory-efficient processing for large files
    """
    temp_audio_path = None
    try:
        # Validate input parameters
        file_provided = file is not None and file.filename is not None and file.filename.strip() != ""
        input_sources = sum([
            file_provided,
            bool(storage_key),
            bool(s3_presigned_url)
        ])
        
        if input_sources == 0:
            raise HTTPException(400, "One input source required: file upload, storage_key, or s3_presigned_url")
        elif input_sources > 1:
            raise HTTPException(400, "Only one input source allowed at a time")
        
        # Output key handling is now done in the storage logic based on stored_output parameter
        
        # Handle legacy parameters if provided (model, temperature) - these are ignored but logged
        form_data = await request.form()
        ignored_params = []
        
        # Check for model parameter (ignored)
        if "model" in form_data:
            model_value = form_data.get("model")
            if model_value != "large-v3-turbo":
                ignored_params.append(f"model={model_value} (using large-v3-turbo)")
        
        # Check for temperature parameter (ignored)
        if "temperature" in form_data:
            temperature_value = form_data.get("temperature")
            try:
                temp_float = float(temperature_value)
                if temp_float != 0.0:
                    ignored_params.append(f"temperature={temperature_value} (using 0.0)")
            except (ValueError, TypeError):
                ignored_params.append(f"temperature={temperature_value} (invalid, using 0.0)")
        
        if ignored_params:
            logger.info(f"â„¹ï¸  Ignored parameters in this request: {', '.join(ignored_params)}")
        
        # Handle different input sources
        if file:
            # Direct file upload (existing functionality)
            logger.info("Processing direct file upload")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
                temp_audio_path = temp_file.name
                shutil.copyfileobj(file.file, temp_file)
                
        elif storage_key:
            # Storage object download from configured bucket
            bucket = DEFAULT_UPLOAD_BUCKET
            logger.info(f"Processing storage object: {bucket}/{storage_key}")
            temp_audio_path = await download_from_r2(bucket, storage_key)
            
        elif s3_presigned_url:
            # Pre-signed S3 URL download
            logger.info(f"Processing pre-signed URL: {s3_presigned_url[:50]}...")
            temp_audio_path = await download_from_s3_presigned(s3_presigned_url)
        
        # ASYNC PROCESSING MODE: If upload_presigned_url is provided, return immediately
        if upload_presigned_url:
            # Generate unique job ID
            job_id = str(uuid.uuid4())
            
            logger.info(f"ðŸš€ Starting async transcription job {job_id}")
            logger.info(f"   Will upload results to user-provided presigned URL")
            
            # Prepare timestamp granularities for response
            granularities = [g.strip().lower() for g in timestamp_granularities.split(',')]
            
            # Start background processing
            background_tasks.add_task(
                process_transcription_background,
                job_id=job_id,
                temp_audio_path=temp_audio_path,
                upload_presigned_url=upload_presigned_url,
                enable_diarization=enable_diarization,
                language=language,
                batch_size=batch_size,
                prompt=prompt,
                timestamp_granularities=timestamp_granularities,
                output_content=output_content,
                clustering_threshold=clustering_threshold,
                segmentation_threshold=segmentation_threshold,
                min_speaker_duration=min_speaker_duration,
                speaker_confidence_threshold=speaker_confidence_threshold,
                speaker_smoothing_enabled=speaker_smoothing_enabled,
                min_switch_duration=min_switch_duration,
                vad_validation_enabled=vad_validation_enabled,
                optimized_alignment=optimized_alignment
            )
            
            # Return immediate response with job metadata
            return {
                "job_id": job_id,
                "status": "in_progress",
                "message": "Transcription job started. Results will be uploaded to your S3 bucket when complete.",
                "processing_info": {
                    "device": server.device,
                    "language": language,
                    "timestamp_granularities": granularities,
                    "diarization_enabled": enable_diarization,
                    "optimized_alignment": optimized_alignment
                },
                "storage_info": {
                    "uploaded": False,
                    "storage_url": upload_presigned_url,
                    "key": "user-specified",
                    "service": "User S3 Bucket",
                    "notification": "Use S3 event notifications to detect when the result file is uploaded"
                }
            }
        
        # SYNCHRONOUS PROCESSING MODE: Process transcription and wait for completion
        result = await server.transcribe_with_diarization(
            temp_audio_path, 
            enable_diarization=enable_diarization,
            language=language,
            model="large-v3-turbo",  # Performance optimized model
            batch_size=batch_size,
            prompt=prompt,
            temperature=0.0,  # Always use 0.0 (ignored parameters handled above)
            timestamp_granularities=timestamp_granularities,
            output_content=output_content,
            clustering_threshold=clustering_threshold,
            segmentation_threshold=segmentation_threshold,
            min_speaker_duration=min_speaker_duration,
            speaker_confidence_threshold=speaker_confidence_threshold,
            speaker_smoothing_enabled=speaker_smoothing_enabled,
            min_switch_duration=min_switch_duration,
            vad_validation_enabled=vad_validation_enabled,
            optimized_alignment=optimized_alignment
        )
        
        # Handle output to storage based on user's requirements:
        # IF stored_output IS TRUE, THEN
        #   IF upload_presigned_URL IS TRUE, THEN output to upload_presigned_URL
        #   ELSE output to downloads bucket (defined in /downloads endpoint)
        # ELSE provide response within API call
        storage_url = None
        
        if stored_output:
            # User wants stored output
            if upload_presigned_url:
                # Priority 1: Upload to user's pre-signed URL
                storage_url = await upload_to_user_presigned_url(result, upload_presigned_url)
                service_name = "User S3 Bucket"
                bucket_info = upload_presigned_url
                key_info = "user-specified"
            else:
                # Priority 2: Upload to downloads bucket (default server storage)
                if not output_key:
                    # Auto-generate output key if not provided
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                    output_key = f"transcriptions/{timestamp}_result.json"
                    logger.info(f"Auto-generated output key: {output_key}")
                
                bucket = DEFAULT_RESULTS_BUCKET
                storage_url = await upload_result_to_storage(
                    result, 
                    bucket, 
                    output_key, 
                    use_r2=True  # Use primary storage (downloads bucket)
                )
                service_name = "Downloads Bucket"
                bucket_info = bucket
                key_info = output_key
        # If stored_output is False, don't store anything - just return API response
        
        # Add storage info to result if upload occurred
        if storage_url:
            # Ensure result has the right structure for adding storage info
            if isinstance(result, dict):
                result["storage_info"] = {
                    "uploaded": True,
                    "storage_url": storage_url,
                    "bucket": bucket_info,
                    "key": key_info,
                    "service": service_name
                }
        
        if response_format == "json":
            return result
        elif response_format == "verbose_json":
            return result  # Return as proper JSON response
        elif response_format == "text":
            # Extract just the text content
            text_content = "\n".join([segment.get('text', '') for segment in result.get('result', {}).get('segments', [])])
            return PlainTextResponse(content=text_content.strip())
        elif response_format == "srt":
            # Generate SRT format
            segments = result.get('result', {}).get('segments', [])
            srt_content = ""
            for i, segment in enumerate(segments, 1):
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                text = segment.get('text', '').strip()
                start_srt = f"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{start_time%60:06.3f}".replace('.', ',')
                end_srt = f"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{end_time%60:06.3f}".replace('.', ',')
                srt_content += f"{i}\n{start_srt} --> {end_srt}\n{text}\n\n"
            return PlainTextResponse(content=srt_content.strip())
        elif response_format == "vtt":
            # Generate WebVTT format
            segments = result.get('result', {}).get('segments', [])
            vtt_content = "WEBVTT\n\n"
            for segment in segments:
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                text = segment.get('text', '').strip()
                start_vtt = f"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{start_time%60:06.3f}"
                end_vtt = f"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{end_time%60:06.3f}"
                vtt_content += f"{start_vtt} --> {end_vtt}\n{text}\n\n"
            return PlainTextResponse(content=vtt_content.strip())
        else:
            # Default: return as JSON
            return result
            
    except Exception as e:
        logger.error(f"Transcription request failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # Cleanup temporary files (from any source: direct upload, storage, or S3)
        if temp_audio_path and os.path.exists(temp_audio_path):
            try:
                os.unlink(temp_audio_path)
                logger.debug(f"Cleaned up temporary audio file: {temp_audio_path}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup temporary file {temp_audio_path}: {cleanup_error}")

if __name__ == "__main__":
    logger.info("ðŸš€ Starting WhisperX Diarization Server with Multi-Cloud Storage")
    logger.info("âœ… FEATURES ENABLED:")
    logger.info("   - Advanced speaker diarization with confidence scoring")
    logger.info("   - Multi-cloud storage support (R2, S3)")
    logger.info("   - GPU-accelerated processing")
    logger.info("   - Automatic result export")
    logger.info("ðŸš€ STORAGE INTEGRATIONS:")
    logger.info("   - Cloudflare R2 direct upload and processing")
    logger.info("   - Pre-signed S3 URL support")
    logger.info("   - Automatic result archiving")
    logger.info("   - Zero-bandwidth cost processing for large files")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=3333,
        log_level="info"
    )