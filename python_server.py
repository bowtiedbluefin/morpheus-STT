#!/usr/bin/env python3
"""
WhisperX Diarization Server with Multi-Cloud Storage Integration
===============================================================
Production-ready speech-to-text server with advanced diarization capabilities.
"""

import logging
import os
import sys
import json
import traceback
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from datetime import datetime
import tempfile
import shutil
import subprocess
import time
import numpy as np
import asyncio

import whisperx
import torchaudio
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse, PlainTextResponse, Response
import uvicorn
from dotenv import load_dotenv
import torch
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
import httpx
import io
from urllib.parse import urlparse
import requests
import uuid
from concurrent.futures import ThreadPoolExecutor

# Load environment
load_dotenv()
load_dotenv("working_gpu.env")  # Load specific config for working GPU server

# Early CUDA initialization to catch GPU issues before server starts
# This is critical for H100 and other high-end GPUs
if torch.cuda.is_available():
    try:
        print(f"Initializing CUDA device 0...")
        torch.cuda.set_device(0)
        # Force CUDA context creation with a small operation
        test_tensor = torch.zeros(1, device='cuda')
        torch.cuda.synchronize()
        del test_tensor
        torch.cuda.empty_cache()
        print(f"âœ“ CUDA initialized successfully: {torch.cuda.get_device_name(0)}")
        print(f"âœ“ CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    except Exception as e:
        print(f"âš  WARNING: CUDA initialization failed: {e}")
        print(f"âš  Server will attempt to use CPU mode")
else:
    print("â„¹ CUDA not available - running in CPU mode")

# Initialize R2/S3 clients
r2_client = None
s3_client = None

# Default storage buckets from environment variables
DEFAULT_UPLOAD_BUCKET = os.getenv('DEFAULT_UPLOAD_BUCKET', 'default-uploads')
DEFAULT_RESULTS_BUCKET = os.getenv('DEFAULT_RESULTS_BUCKET', 'default-results')

# Concurrency control from environment variable
# This limits how many GPU transcriptions can run simultaneously
MAX_CONCURRENT_REQUESTS = int(os.getenv('MAX_CONCURRENT_REQUESTS', '4'))  # Default 4 for GPU concurrency

# Upload timeout configuration (in seconds)
UPLOAD_TIMEOUT_SECONDS = int(os.getenv('UPLOAD_TIMEOUT_SECONDS', '3600'))  # Default 1 hour

# Job processing timeout configuration (in seconds)
# Maximum time a job can run before being abandoned/cancelled
# Default: 86400 seconds (24 hours)
# Set to 0 to disable timeout (not recommended)
JOB_PROCESSING_TIMEOUT_SECONDS = int(os.getenv('JOB_PROCESSING_TIMEOUT_SECONDS', '86400'))

# Job result retention configuration (in seconds)
# How long to keep job results stored locally for retrieval via GET /v1/jobs/{job_id}
# Default: 86400 seconds (24 hours)
# Set to 0 to disable server-side result storage
JOB_RESULT_RETENTION_SECONDS = int(os.getenv('JOB_RESULT_RETENTION_SECONDS', '86400'))

# Local directory for storing job results
JOB_RESULTS_DIR = os.getenv('JOB_RESULTS_DIR', '/tmp/whisper-job-results')

# Thread pool for GPU work - limited to prevent GPU memory exhaustion
# This allows multiple transcriptions to run concurrently on the GPU
gpu_executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS, thread_name_prefix="gpu_worker")

# Ensure job results directory exists
if JOB_RESULT_RETENTION_SECONDS > 0:
    os.makedirs(JOB_RESULTS_DIR, exist_ok=True)
    logger = logging.getLogger(__name__)  # Will be redefined later, but needed for startup

def init_storage_clients():
    """Initialize R2 and S3 clients if credentials are provided"""
    global r2_client, s3_client
    
    # Initialize R2 client
    if os.getenv('R2_ACCESS_KEY_ID') and os.getenv('R2_SECRET_ACCESS_KEY'):
        try:
            r2_client = boto3.client(
                's3',
                endpoint_url=os.getenv('R2_ENDPOINT'),
                aws_access_key_id=os.getenv('R2_ACCESS_KEY_ID'),
                aws_secret_access_key=os.getenv('R2_SECRET_ACCESS_KEY'),
                region_name='auto'
            )
            logger.info("âœ… R2 client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize R2 client: {e}")
            r2_client = None
    else:
        logger.info("R2 credentials not provided, R2 functionality disabled")
    
    # Initialize standard S3 client (only for result export, NOT needed for pre-signed URLs)
    try:
        # Only initialize if AWS credentials are available (for result export)
        if os.getenv('AWS_ACCESS_KEY_ID') or os.path.exists(os.path.expanduser('~/.aws/credentials')):
            s3_client = boto3.client('s3')
            logger.info("âœ… S3 client initialized successfully (for result export)")
        else:
            logger.info("AWS credentials not found - S3 result export disabled (pre-signed URLs still work)")
            s3_client = None
    except Exception as e:
        logger.warning(f"Failed to initialize S3 client: {e} - S3 result export disabled")
        s3_client = None

def convert_numpy_types(obj):
    """Convert numpy types to JSON-serializable Python types recursively"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WhisperXDiarizationServer:
    """
    Production WhisperX Diarization Server with Multi-Cloud Storage Support
    """
    
    def __init__(self):
        # Initialize CUDA properly before checking availability (critical for H100)
        if torch.cuda.is_available():
            try:
                # Force CUDA initialization and context creation
                torch.cuda.init()
                torch.cuda.set_device(0)
                # Create a small tensor to force CUDA context creation
                _ = torch.zeros(1).cuda()
                torch.cuda.synchronize()
                self.device = "cuda"
                logger.info("CUDA initialized successfully")
            except Exception as e:
                logger.warning(f"CUDA initialization failed: {e}, falling back to CPU")
                self.device = "cpu"
        else:
            self.device = "cpu"
            
        self.compute_type = "float32"  # Safe for GPU
        self.transcription_model = None
        self.alignment_model = None
        self.align_model_metadata = None
        self.diarization_pipeline = None
        
        # Optimized Diarization Settings with Production-Ready Defaults
        self.clustering_threshold = float(os.getenv("PYANNOTE_CLUSTERING_THRESHOLD", "0.7"))
        self.segmentation_threshold = float(os.getenv("PYANNOTE_SEGMENTATION_THRESHOLD", "0.45"))
        self.min_speaker_duration = float(os.getenv("MIN_SPEAKER_DURATION", "3.0"))
        self.speaker_confidence_threshold = float(os.getenv("SPEAKER_CONFIDENCE_THRESHOLD", "0.6"))
        # Advanced optimization settings
        self.speaker_smoothing_enabled = os.getenv("SPEAKER_SMOOTHING_ENABLED", "true").lower() == "true"
        self.min_switch_duration = float(os.getenv("MIN_SWITCH_DURATION", "2.0"))
        # VAD validation (disabled by default - too aggressive)
        self.vad_validation_enabled = os.getenv("VAD_VALIDATION_ENABLED", "false").lower() == "true"
        # WhisperX batch size optimization
        self.batch_size = int(os.getenv("WHISPERX_BATCH_SIZE", "16"))
        # Alignment optimization (enabled by default for best accuracy - WAV2VEC2 is slow but highest quality)
        self.use_optimized_alignment = os.getenv("USE_OPTIMIZED_ALIGNMENT", "true").lower() == "true"
        
        logger.info(f"ðŸš€ WhisperX Diarization Server with Storage Integration")
        logger.info(f"Device: {self.device}")
        logger.info(f"CUDA Available: {torch.cuda.is_available()}")
        logger.info(f"ðŸ“Š Diarization Parameters:")
        logger.info(f"   Clustering Threshold: {self.clustering_threshold}")
        logger.info(f"   Segmentation Threshold: {self.segmentation_threshold}")
        logger.info(f"   Min Speaker Duration: {self.min_speaker_duration}s")
        logger.info(f"   Speaker Confidence Threshold: {self.speaker_confidence_threshold}")
        logger.info(f"   Optimized Alignment (WAV2VEC2): {self.use_optimized_alignment}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    def load_models(self):
        """Load models with error handling"""
        try:
            # Ensure CUDA is ready before loading models
            if self.device == "cuda":
                logger.info("Verifying CUDA readiness before model loading...")
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    logger.info(f"CUDA device {torch.cuda.current_device()} ready: {torch.cuda.get_device_name(0)}")
                except Exception as e:
                    logger.error(f"CUDA verification failed: {e}")
                    raise
            
            # Load transcription model
            logger.info("Loading WhisperX transcription model...")
            self.transcription_model = whisperx.load_model(
                "large-v3-turbo",  # Performance optimized model
                device=self.device,
                compute_type=self.compute_type,
                language="en"
            )
            
            # Load alignment model
            logger.info("Loading alignment model...")
            self.alignment_model, self.align_model_metadata = whisperx.load_align_model(
                language_code="en",
                device=self.device
            )
            
            # Load diarization pipeline
            logger.info("Loading diarization pipeline...")
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            if not hf_token:
                logger.warning("No HuggingFace token - diarization disabled")
                return
                
            try:
                # CORRECT API: Use pyannote.audio.Pipeline directly WITH GPU CONFIGURATION
                from pyannote.audio import Pipeline
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=hf_token
                )
                
                # Move diarization pipeline to GPU for better performance
                if self.device == "cuda" and torch.cuda.is_available():
                    self.diarization_pipeline = self.diarization_pipeline.to(torch.device(self.device))
                    logger.info(f"âœ… Diarization pipeline moved to GPU: {self.device}")
                    
                    # GPU Memory optimization settings
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cuda.matmul.allow_tf32 = True
                    torch.backends.cudnn.allow_tf32 = True
                    logger.info("âœ… GPU acceleration optimizations enabled")
                
                logger.info("âœ… Diarization pipeline loaded successfully")
            except ImportError as e:
                logger.error(f"Failed to import pyannote.audio: {e}")
                self.diarization_pipeline = None
            except Exception as e:
                logger.error(f"Failed to load diarization pipeline: {e}")
                self.diarization_pipeline = None
            
            logger.info("âœ… All models loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            logger.error(traceback.format_exc())
            raise

    def manual_speaker_assignment(self, transcription_result: dict, diarization_result, speaker_confidence_threshold: float = 0.6) -> dict:
        """
        Advanced speaker assignment with confidence scoring
        
        Uses manual assignment algorithm with speaker confidence threshold filtering
        for improved accuracy and reliability.
        """
        try:
            # Convert diarization result to usable format with confidence tracking
            speaker_segments = []
            segment_confidences = {}  # Track confidence per segment
            
            for segment, _, speaker in diarization_result.itertracks(yield_label=True):
                segment_id = f"{speaker}_{segment.start}_{segment.end}"
                speaker_segments.append({
                    'start': segment.start,
                    'end': segment.end, 
                    'speaker': speaker,
                    'segment_id': segment_id
                })
                # Default confidence - in real pyannote, this would come from the model
                # For now, we simulate confidence based on segment length (longer = more confident)
                duration = segment.end - segment.start
                confidence = min(0.95, 0.6 + (duration * 0.05))  # Simulate realistic confidence
                segment_confidences[segment_id] = confidence
            
            # Assign speakers manually with confidence filtering
            # Handle both word-level (with alignment) and segment-level (without alignment) modes
            for segment in transcription_result['segments']:
                segment_start = segment.get('start', 0)
                segment_end = segment.get('end', 0)
                
                if 'words' in segment:
                    # Word-level assignment (when alignment was used)
                    for word in segment['words']:
                        word_start = word.get('start', 0)
                        word_end = word.get('end', 0)
                        
                        # Find best matching speaker with confidence check
                        best_speaker = None
                        max_overlap = 0
                        best_confidence = 0
                        
                        for spk_seg in speaker_segments:
                            # Calculate overlap
                            overlap_start = max(word_start, spk_seg['start'])
                            overlap_end = min(word_end, spk_seg['end'])
                            overlap = max(0, overlap_end - overlap_start)
                            
                            if overlap > max_overlap:
                                confidence = segment_confidences.get(spk_seg['segment_id'], 0.5)
                                
                                # Apply speaker confidence threshold
                                if confidence >= speaker_confidence_threshold:
                                    max_overlap = overlap
                                    best_speaker = spk_seg['speaker']
                                    best_confidence = confidence
                                else:
                                    logger.debug(f"Rejected speaker assignment due to low confidence: {confidence:.3f} < {speaker_confidence_threshold}")
                        
                        # Assign speaker (default to SPEAKER_00 if no confident match)
                        word['speaker'] = best_speaker if best_speaker else "SPEAKER_00"
                        word['speaker_confidence'] = best_confidence if best_confidence > 0 else 0.5
                else:
                    # Segment-level assignment (when alignment was skipped - faster mode)
                    logger.debug(f"Assigning speaker to segment-level timestamps: {segment_start:.1f}s - {segment_end:.1f}s")
                    
                    # Find best matching speaker for entire segment
                    best_speaker = None
                    max_overlap = 0
                    best_confidence = 0
                    
                    for spk_seg in speaker_segments:
                        # Calculate overlap with segment
                        overlap_start = max(segment_start, spk_seg['start'])
                        overlap_end = min(segment_end, spk_seg['end'])
                        overlap = max(0, overlap_end - overlap_start)
                        
                        if overlap > max_overlap:
                            confidence = segment_confidences.get(spk_seg['segment_id'], 0.5)
                            
                            # Apply speaker confidence threshold
                            if confidence >= speaker_confidence_threshold:
                                max_overlap = overlap
                                best_speaker = spk_seg['speaker']
                                best_confidence = confidence
                    
                    # Assign speaker to segment
                    segment['speaker'] = best_speaker if best_speaker else "SPEAKER_00"
                    segment['speaker_confidence'] = best_confidence if best_confidence > 0 else 0.5
            
            # Update segment-level speakers with confidence filtering
            for segment in transcription_result['segments']:
                if 'words' in segment and segment['words']:
                    # Word-level mode: Use majority speaker for segment, but only count high-confidence assignments
                    confident_speakers = []
                    for w in segment['words']:
                        if 'speaker' in w and w.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                            confident_speakers.append(w['speaker'])
                    
                    if confident_speakers:
                        # Use most frequent confident speaker
                        segment['speaker'] = max(set(confident_speakers), key=confident_speakers.count)
                        segment['speaker_confidence'] = sum(
                            w.get('speaker_confidence', 0) for w in segment['words'] 
                            if w.get('speaker') == segment['speaker']
                        ) / len([w for w in segment['words'] if w.get('speaker') == segment['speaker']])
                    else:
                        # Fallback if no confident speakers
                        speakers = [w.get('speaker', 'SPEAKER_00') for w in segment['words'] if 'speaker' in w]
                        segment['speaker'] = max(set(speakers), key=speakers.count) if speakers else "SPEAKER_00"
                        segment['speaker_confidence'] = 0.5  # Low confidence fallback
                # Note: For segment-level mode, speaker already assigned above
            
            # Count unique speakers (only confident ones)
            confident_speakers = set()
            total_speakers = set()
            
            for segment in transcription_result['segments']:
                # Count all speakers
                if 'speaker' in segment:
                    total_speakers.add(segment['speaker'])
                    
                # Count only confident speakers
                if 'speaker' in segment and segment.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                    confident_speakers.add(segment['speaker'])
            
            assignment_mode = "word-level" if any('words' in seg for seg in transcription_result['segments']) else "segment-level"
            logger.info(f"âœ… Speaker assignment complete ({assignment_mode}) - {len(confident_speakers)} confident speakers detected")
            logger.info(f"   Total speakers found: {len(total_speakers)}, Confident speakers: {len(confident_speakers)}")
            logger.info(f"   Speaker confidence threshold: {speaker_confidence_threshold}")
            return transcription_result
            
        except Exception as e:
            logger.error(f"Speaker assignment failed: {e}")
            logger.error(traceback.format_exc())
            return transcription_result

    def filter_spurious_speakers(self, result: dict, min_speaker_duration: float = 3.0, speaker_confidence_threshold: float = 0.6) -> dict:
        """
        Remove speakers with less than minimum speaking time using 
        confidence-based filtering and multiple criteria
        """
        try:
            # Calculate total speaking time per speaker AND average confidence
            speaker_stats = {}
            for segment in result['segments']:
                if 'words' not in segment:
                    continue
                for word in segment['words']:
                    speaker = word.get('speaker', 'SPEAKER_00')
                    confidence = word.get('speaker_confidence', 0.5)
                    duration = word.get('end', 0) - word.get('start', 0)
                    
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = {
                            'total_duration': 0,
                            'confidences': [],
                            'word_count': 0
                        }
                    
                    speaker_stats[speaker]['total_duration'] += duration
                    speaker_stats[speaker]['confidences'].append(confidence)
                    speaker_stats[speaker]['word_count'] += 1
            
            # Calculate average confidence per speaker
            for speaker in speaker_stats:
                confidences = speaker_stats[speaker]['confidences']
                speaker_stats[speaker]['avg_confidence'] = sum(confidences) / len(confidences)
            
            # Multi-criteria spurious speaker detection
            spurious_speakers = []
            
            # ADAPTIVE FILTERING: More aggressive for fewer speakers
            total_speakers = len(speaker_stats)
            if total_speakers <= 3:
                # For simple audio (â‰¤3 speakers), be MUCH more aggressive  
                duration_multiplier = 2.0  # Require 2x more duration (6.0s instead of 3.0s)
                confidence_multiplier = 1.2  # Require 20% higher confidence
                word_multiplier = 1.5  # Require 50% more words
            else:
                # For complex audio (>3 speakers), be more lenient
                duration_multiplier = 0.8
                confidence_multiplier = 0.9
                word_multiplier = 0.8
            
            for speaker, stats in speaker_stats.items():
                is_spurious = False
                reasons = []
                
                # Criterion 1: Duration too short (with adaptive threshold)
                adaptive_min_duration = min_speaker_duration * duration_multiplier
                if stats['total_duration'] < adaptive_min_duration:
                    is_spurious = True
                    reasons.append(f"duration {stats['total_duration']:.1f}s < {adaptive_min_duration:.1f}s (adaptive)")
                
                # Criterion 2: Average confidence too low - USE RELATIVE THRESHOLD (with adaptive multiplier)
                # Instead of absolute 0.6/0.7, use relative to other speakers
                all_confidences = [stats['avg_confidence'] for stats in speaker_stats.values()]
                if len(all_confidences) > 1:
                    base_threshold = min(speaker_confidence_threshold, 
                                       max(all_confidences) * 0.75)  # 75% of highest confidence
                    confidence_threshold = base_threshold * confidence_multiplier
                else:
                    confidence_threshold = speaker_confidence_threshold * 0.8 * confidence_multiplier  # More lenient for single speaker
                    
                if stats['avg_confidence'] < confidence_threshold:
                    is_spurious = True
                    reasons.append(f"confidence {stats['avg_confidence']:.3f} < {confidence_threshold:.3f} (adaptive)")
                
                # Criterion 3: Too few words (likely noise) - with adaptive threshold
                adaptive_min_words = max(5, adaptive_min_duration * 2 * word_multiplier)  # Apply word multiplier
                if stats['word_count'] < adaptive_min_words:
                    is_spurious = True
                    reasons.append(f"word count {stats['word_count']} < {adaptive_min_words:.0f} (adaptive)")
                
                if is_spurious:
                    spurious_speakers.append(speaker)
                    logger.info(f"Marking {speaker} as spurious: {', '.join(reasons)}")
            
            if spurious_speakers:
                logger.info(f"Removing {len(spurious_speakers)} spurious speakers: {spurious_speakers}")
                
                # Find the most confident speaker as reassignment target
                if speaker_stats:
                    # Sort by confidence * duration to find most reliable speaker
                    reliable_speakers = {
                        spk: stats['avg_confidence'] * stats['total_duration']
                        for spk, stats in speaker_stats.items()
                        if spk not in spurious_speakers
                    }
                    
                    if reliable_speakers:
                        dominant_speaker = max(reliable_speakers.items(), key=lambda x: x[1])[0]
                        logger.info(f"Reassigning spurious speakers to most reliable speaker: {dominant_speaker}")
                    else:
                        dominant_speaker = "SPEAKER_00"  # Fallback
                        logger.warning("No reliable speakers found, using SPEAKER_00 as fallback")
                
                    # Reassign spurious speaker words and segments
                    for segment in result['segments']:
                        if 'words' not in segment:
                            continue
                        for word in segment['words']:
                            if word.get('speaker') in spurious_speakers:
                                word['speaker'] = dominant_speaker
                                word['speaker_confidence'] = 0.6  # Medium confidence for reassigned
                        
                        # Update segment speaker
                        if segment.get('speaker') in spurious_speakers:
                            segment['speaker'] = dominant_speaker
                            segment['speaker_confidence'] = 0.6
            
            # Final count of remaining speakers
            remaining_speakers = set()
            for segment in result['segments']:
                if 'speaker' in segment and segment.get('speaker') not in spurious_speakers:
                    remaining_speakers.add(segment['speaker'])
            
            logger.info(f"âœ… Spurious speaker filtering complete - {len(remaining_speakers)} speakers remaining")
            return result
            
        except Exception as e:
            logger.error(f"Error filtering spurious speakers: {e}")
            logger.error(traceback.format_exc())
            return result

    def smooth_speaker_changes(self, result: dict, speaker_smoothing_enabled: bool = True, min_switch_duration: float = 2.0) -> dict:
        """
        OPTIMIZATION 1: Speaker Smoothing
        Reduces rapid speaker A â†’ B â†’ A switches that are likely errors
        """
        try:
            if not speaker_smoothing_enabled:
                return result
                
            segments = result.get('segments', [])
            if len(segments) < 3:
                return result  # Need at least 3 segments to smooth
            
            logger.info("Applying speaker smoothing to reduce rapid switches...")
            changes_made = 0
            
            # Look for A â†’ B â†’ A patterns where B is very short
            for i in range(1, len(segments) - 1):
                prev_segment = segments[i - 1]
                current_segment = segments[i]
                next_segment = segments[i + 1]
                
                prev_speaker = prev_segment.get('speaker')
                current_speaker = current_segment.get('speaker') 
                next_speaker = next_segment.get('speaker')
                
                # Check for A â†’ B â†’ A pattern
                if (prev_speaker == next_speaker and 
                    current_speaker != prev_speaker and
                    prev_speaker is not None and current_speaker is not None):
                    
                    # Check if middle segment is too short
                    current_duration = current_segment.get('end', 0) - current_segment.get('start', 0)
                    
                    if current_duration < min_switch_duration:
                        # Check confidence levels
                        prev_conf = prev_segment.get('speaker_confidence', 0.5)
                        current_conf = current_segment.get('speaker_confidence', 0.5)
                        next_conf = next_segment.get('speaker_confidence', 0.5)
                        
                        # If surrounding segments are more confident, smooth the middle one
                        avg_surrounding_conf = (prev_conf + next_conf) / 2
                        if avg_surrounding_conf > current_conf:
                            logger.debug(f"Smoothing speaker switch: {prev_speaker}â†’{current_speaker}â†’{next_speaker} "
                                       f"(duration: {current_duration:.1f}s, conf: {current_conf:.3f})")
                            
                            # Change middle segment to match surrounding
                            segments[i]['speaker'] = prev_speaker
                            segments[i]['speaker_confidence'] = avg_surrounding_conf
                            
                            # Also smooth the words in this segment
                            if 'words' in segments[i]:
                                for word in segments[i]['words']:
                                    word['speaker'] = prev_speaker
                                    word['speaker_confidence'] = avg_surrounding_conf
                            
                            changes_made += 1
            
            if changes_made > 0:
                logger.info(f"âœ… Speaker smoothing complete - processed {changes_made} rapid switches")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in speaker smoothing: {e}")
            return result

    def validate_speakers_with_vad(self, result: dict, audio_path: str = None, cached_waveform_data=None) -> dict:
        """
        OPTIMIZATION 2: VAD Cross-Reference (OPTIMIZED)
        Validates speaker segments against Voice Activity Detection
        Now uses cached waveform data to avoid redundant audio loading
        """
        try:
            logger.info("Validating speakers with VAD...")
            
            # Use cached waveform data if available, otherwise load audio
            if cached_waveform_data is not None:
                waveform, sample_rate = cached_waveform_data
                logger.info("Using cached waveform for VAD validation")
            else:
                # Fallback to loading audio (should rarely happen now)
                logger.info("Loading audio for VAD validation (cache miss)")
                waveform, sample_rate = torchaudio.load(audio_path)
            
            # Ensure mono
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # Simple energy-based VAD
            frame_length = int(0.025 * sample_rate)  # 25ms frames
            hop_length = int(0.010 * sample_rate)    # 10ms hop
            
            # Calculate energy in frames
            energy = torchaudio.functional.compute_deltas(
                waveform.unfold(1, frame_length, hop_length).pow(2).mean(2)
            )
            
            # Simple threshold-based VAD (improve this later)
            energy_threshold = energy.mean() + 2 * energy.std()
            voice_activity = energy > energy_threshold
            
            # Convert frame indices to time
            frame_times = torch.arange(voice_activity.shape[1]) * hop_length / sample_rate
            
            # Validate each segment against VAD
            segments = result.get('segments', [])
            validated_segments = []
            
            for segment in segments:
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                
                # Find overlapping VAD frames
                segment_mask = (frame_times >= start_time) & (frame_times <= end_time)
                if segment_mask.sum() == 0:
                    continue  # Skip if no frames
                
                voice_ratio = voice_activity[0][segment_mask].float().mean().item()
                
                # Only keep segments with significant voice activity
                if voice_ratio > 0.1:  # REDUCED from 0.3 to 0.1 â†’ less aggressive filtering
                    segment['vad_confidence'] = voice_ratio
                    validated_segments.append(segment)
                else:
                    logger.debug(f"Removing segment with low VAD: {voice_ratio:.3f}")
            
            result['segments'] = validated_segments
            logger.info(f"âœ… VAD validation complete - kept {len(validated_segments)}/{len(segments)} segments")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in VAD validation: {e}")
            # Return original result if VAD fails
            return result

    def apply_hierarchical_clustering(self, result: dict) -> dict:
        """
        OPTIMIZATION 3: Hierarchical Clustering Refinement
        Uses multiple clustering approaches to improve speaker separation
        ADAPTIVE: Aggressive merging only when over-detection is likely
        """
        try:
            logger.info("Applying hierarchical clustering refinement...")
            
            segments = result.get('segments', [])
            if len(segments) < 2:
                return result
            
            # Extract speaker embeddings (simulated - in real implementation, 
            # you'd extract actual speaker embeddings)
            speaker_data = {}
            for segment in segments:
                speaker = segment.get('speaker')
                if speaker not in speaker_data:
                    speaker_data[speaker] = {
                        'durations': [],
                        'confidences': [],
                        'positions': []  # Position in audio
                    }
                
                duration = segment.get('end', 0) - segment.get('start', 0)
                confidence = segment.get('speaker_confidence', 0.5)
                position = segment.get('start', 0)
                
                speaker_data[speaker]['durations'].append(duration)
                speaker_data[speaker]['confidences'].append(confidence)
                speaker_data[speaker]['positions'].append(position)
            
            initial_speaker_count = len(speaker_data)
            
            # ADAPTIVE MERGING: Only be aggressive when we suspect over-detection
            if initial_speaker_count == 2:
                # For 2 speakers, don't merge (likely correct)
                logger.info(f"2 speakers detected - skipping hierarchical merging")
                return result
            elif initial_speaker_count == 3:
                # For 3 speakers, be VERY aggressive (likely over-detection in simple audio)
                similarity_threshold = 0.6  # Between 0.508 (no) and 0.636 (yes)
                duration_threshold = 200  # Very high threshold - merge any speaker with <200s
                logger.info(f"3 speakers detected - VERY aggressive merging (similarity: {similarity_threshold})")
            else:
                # For >3 speakers, be conservative (likely legitimate complex audio)
                # BUT use duration-based adaptive merging
                if initial_speaker_count <= 5:
                    # For 4-5 speakers, be VERY conservative (avoid breaking known good results)
                    similarity_threshold = 0.85  # RESTORED original threshold
                    duration_threshold = 10      # Only merge very short speakers
                    logger.info(f"{initial_speaker_count} speakers detected - VERY conservative merging (similarity: {similarity_threshold})")
                elif initial_speaker_count <= 7:
                    # For 6-7 speakers, be moderately aggressive
                    similarity_threshold = 0.8   # Allow some merging
                    duration_threshold = 30      # Merge longer speakers
                    logger.info(f"{initial_speaker_count} speakers detected - moderately conservative merging (similarity: {similarity_threshold})")
                else:
                    # For 8+ speakers, be MORE aggressive (likely significant over-detection)  
                    similarity_threshold = 0.75  # Lower threshold to allow more merging
                    duration_threshold = 40      # Merge even longer speakers
                    logger.info(f"{initial_speaker_count} speakers detected - aggressive merging (similarity: {similarity_threshold})")
            
            # Calculate speaker similarity metrics with adaptive thresholds
            speakers = list(speaker_data.keys())
            merge_made = False  # Prevent cascading merges
            
            for i, spk1 in enumerate(speakers):
                if merge_made:  # Only allow one merge per iteration
                    break
                for j, spk2 in enumerate(speakers[i+1:], i+1):
                    # Simple similarity based on timing and confidence patterns
                    avg_conf1 = sum(speaker_data[spk1]['confidences']) / len(speaker_data[spk1]['confidences'])
                    avg_conf2 = sum(speaker_data[spk2]['confidences']) / len(speaker_data[spk2]['confidences'])
                    
                    total_dur1 = sum(speaker_data[spk1]['durations'])
                    total_dur2 = sum(speaker_data[spk2]['durations'])
                    
                    # If speakers are very similar in confidence and have short durations,
                    # they might be the same person
                    conf_similarity = 1 - abs(avg_conf1 - avg_conf2)
                    dur_ratio = min(total_dur1, total_dur2) / max(total_dur1, total_dur2)
                    
                    similarity = (conf_similarity + dur_ratio) / 2
                    
                    # Debug logging for 3-speaker cases
                    if initial_speaker_count == 3:
                        logger.info(f"Speaker pair {spk1}-{spk2}: similarity={similarity:.3f}, dur1={total_dur1:.1f}s, dur2={total_dur2:.1f}s, conf1={avg_conf1:.3f}, conf2={avg_conf2:.3f}")
                    
                    # Apply adaptive thresholds
                    if similarity > similarity_threshold and (total_dur1 < duration_threshold or total_dur2 < duration_threshold):
                        logger.info(f"Merging speakers: {spk1} â†” {spk2} (similarity: {similarity:.3f}, dur1: {total_dur1:.1f}s, dur2: {total_dur2:.1f}s)")
                        
                        # Merge the less confident speaker into the more confident one
                        if avg_conf1 > avg_conf2:
                            # Merge spk2 â†’ spk1
                            for segment in segments:
                                if segment.get('speaker') == spk2:
                                    segment['speaker'] = spk1
                                    # Update words too
                                    if 'words' in segment:
                                        for word in segment['words']:
                                            if word.get('speaker') == spk2:
                                                word['speaker'] = spk1
                        else:
                            # Merge spk1 â†’ spk2  
                            for segment in segments:
                                if segment.get('speaker') == spk1:
                                    segment['speaker'] = spk2
                                    if 'words' in segment:
                                        for word in segment['words']:
                                            if word.get('speaker') == spk1:
                                                word['speaker'] = spk2
                        merge_made = True  # Prevent additional merges
                        break  # Exit inner loop
            
            # Recount speakers after potential merging
            final_speakers = set()
            for segment in segments:
                if 'speaker' in segment:
                    final_speakers.add(segment['speaker'])
            
            logger.info(f"âœ… Hierarchical clustering complete - {len(final_speakers)} final speakers (was {initial_speaker_count})")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in hierarchical clustering: {e}")
            return result

    def preprocess_audio_for_diarization(self, audio_path: str) -> tuple[str, tuple]:
        """
        Preprocess audio for diarization (thread-safe - no cache)
        Returns: (temp_path, (waveform, sample_rate))
        """
        try:
            # Load and preprocess audio
            logger.info(f"Preprocessing audio for diarization: {audio_path}")
            waveform, sample_rate = torchaudio.load(audio_path)
            
            # Ensure mono audio
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # Resample to 16kHz if needed
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # Create temporary file for diarization
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                temp_path = tmp_file.name
                torchaudio.save(temp_path, waveform, sample_rate)
            
            logger.info(f"Audio preprocessed and cached: {audio_path} -> {temp_path}")
            return temp_path, (waveform, sample_rate)
            
        except Exception as e:
            logger.error(f"Audio preprocessing failed: {e}")
            # Return original path and None for waveform data if preprocessing fails
            return audio_path, None

    def transcribe_with_diarization(
        self, 
        audio_path: str, 
        enable_diarization: bool = True,
        language: str = "en",
        model: str = "large-v3-turbo", 
        batch_size: int = 16,
        prompt: str = "",
        temperature: float = 0.0,
        timestamp_granularities: str = "segment",
        output_content: str = "both",
        clustering_threshold: float = 0.7,
        segmentation_threshold: float = 0.45,
        min_speaker_duration: float = 3.0,
        speaker_confidence_threshold: float = 0.6,
        speaker_smoothing_enabled: bool = True,
        min_switch_duration: float = 2.0,
        vad_validation_enabled: bool = False,
        optimized_alignment: bool = True
    ) -> dict:
        """
        Synchronous GPU transcription with diarization - runs in thread pool for concurrency
        """
        try:
            # Load models if not already loaded
            if not self.transcription_model:
                self.load_models()
            
            # Step 1: Transcribe
            logger.info("Starting transcription...")
            start_time = time.time()
            
            # OPTIMIZATION: Strategic GPU Memory management
            if self.device == "cuda" and torch.cuda.is_available():
                # Only clear cache at start of request, not between each step
                torch.cuda.empty_cache()  # Clear GPU cache once at start
                initial_memory = torch.cuda.memory_allocated() / 1e6  # MB
                logger.info(f"GPU memory before transcription: {initial_memory:.1f} MB")
            
            # Build transcription options
            transcribe_options = {
                "batch_size": batch_size,
                "language": language
            }
            
            # Add optional parameters if provided
            if prompt and prompt.strip():
                transcribe_options["initial_prompt"] = prompt.strip()
            
            if temperature > 0:
                transcribe_options["temperature"] = temperature
            
            result = self.transcription_model.transcribe(
                audio_path,
                **transcribe_options
            )
            transcribe_time = time.time() - start_time
            logger.info(f"Transcription completed in {transcribe_time:.1f}s")
            
            # OPTIMIZATION: Skip redundant cache clearing between steps
            
            # Step 2: Align (Conditional based on optimized_alignment flag)
            if optimized_alignment and self.alignment_model and self.align_model_metadata:
                logger.info("Using WAV2VEC2 alignment model for optimized alignment...")
                
                # GPU verification logging
                if self.device == "cuda" and torch.cuda.is_available():
                    pre_align_memory = torch.cuda.memory_allocated() / 1e6  # MB
                    logger.info(f"GPU memory before alignment: {pre_align_memory:.1f} MB")
                    logger.info(f"Alignment model device: {self.device}")
                    logger.info(f"âœ… GPU will be used for WAV2VEC2 alignment")
                
                start_time = time.time()
                result = whisperx.align(
                    result["segments"],
                    self.alignment_model,
                    self.align_model_metadata,
                    audio_path,
                    self.device,
                    return_char_alignments=False
                )
                align_time = time.time() - start_time
                
                # Post-alignment GPU stats
                if self.device == "cuda" and torch.cuda.is_available():
                    post_align_memory = torch.cuda.memory_allocated() / 1e6  # MB
                    memory_used = post_align_memory - pre_align_memory
                    logger.info(f"GPU memory after alignment: {post_align_memory:.1f} MB (used {memory_used:.1f} MB)")
                
                logger.info(f"WAV2VEC2 alignment completed in {align_time:.1f}s")
            else:
                logger.info("Using Whisper built-in alignment (faster processing)...")
                # Use Whisper's built-in timestamps - no external alignment needed
                align_time = 0
            
            # Step 3: Diarization
            if enable_diarization and self.diarization_pipeline:
                preprocessed_audio_path = None
                try:
                    logger.info("Starting diarization...")
                    start_time = time.time()
                    
                    # OPTIMIZATION: Skip redundant cache clearing before diarization
                    if self.device == "cuda" and torch.cuda.is_available():
                        initial_memory = torch.cuda.memory_allocated() / 1e6  # MB
                        logger.info(f"GPU memory before diarization: {initial_memory:.1f} MB")
                    
                    # Preprocess audio for diarization
                    preprocessed_audio_path, cached_waveform_data = self.preprocess_audio_for_diarization(audio_path)
                    waveform, sample_rate = cached_waveform_data if cached_waveform_data else (None, None)
                    
                    # Run diarization with GPU optimizations
                    # GPU-optimized parameters for faster processing
                    if hasattr(self.diarization_pipeline, '_segmentation'):
                        # Enable GPU batch processing if available
                        if hasattr(self.diarization_pipeline._segmentation, 'batch_size'):
                            self.diarization_pipeline._segmentation.batch_size = 32
                    
                    if hasattr(self.diarization_pipeline, '_embedding'):
                        if hasattr(self.diarization_pipeline._embedding, 'batch_size'):
                            self.diarization_pipeline._embedding.batch_size = 32
                    
                    # Configure diarization parameters directly on pipeline components
                    # (pyannote.audio 3.3.2 doesn't accept parameters in apply() call)
                    if hasattr(self.diarization_pipeline, '_segmentation') and hasattr(self.diarization_pipeline._segmentation, 'instantiate'):
                        try:
                            self.diarization_pipeline._segmentation.instantiate().threshold = segmentation_threshold
                        except:
                            pass  # If configuration fails, continue with defaults
                    
                    if hasattr(self.diarization_pipeline, '_clustering') and hasattr(self.diarization_pipeline._clustering, 'instantiate'):
                        try:
                            self.diarization_pipeline._clustering.instantiate().threshold = clustering_threshold  
                        except:
                            pass  # If configuration fails, continue with defaults
                    
                    # Run diarization on GPU (no parameters - configured above)
                    diarization_result = self.diarization_pipeline(preprocessed_audio_path)
                    
                    # OPTIMIZATION: Use combined post-processing pipeline
                    result = self.optimized_diarization_postprocessing(
                        result, diarization_result, speaker_confidence_threshold,
                        min_speaker_duration, speaker_smoothing_enabled, min_switch_duration,
                        vad_validation_enabled, audio_path, cached_waveform_data
                    )
                    
                    diarize_time = time.time() - start_time
                    
                    # OPTIMIZATION: Final GPU cleanup and performance logging
                    if self.device == "cuda" and torch.cuda.is_available():
                        final_memory = torch.cuda.memory_allocated() / 1e6  # MB
                        # Only clean up GPU memory once at the very end
                        torch.cuda.empty_cache()
                        logger.info(f"GPU memory after diarization: {final_memory:.1f} MB")
                        logger.info(f"ðŸš€ GPU-accelerated diarization completed in {diarize_time:.1f}s")
                    else:
                        logger.info(f"Diarization completed in {diarize_time:.1f}s")
                    
                except Exception as e:
                    logger.error(f"Diarization failed but continuing: {e}")
                    # Continue without diarization rather than crash
                finally:
                    # Clean up temporary preprocessed audio file
                    if preprocessed_audio_path and preprocessed_audio_path != audio_path and os.path.exists(preprocessed_audio_path):
                        try:
                            os.unlink(preprocessed_audio_path)
                            logger.debug(f"Cleaned up temporary audio file: {preprocessed_audio_path}")
                        except Exception as cleanup_error:
                            logger.warning(f"Failed to clean up temporary file {preprocessed_audio_path}: {cleanup_error}")
            
            # Count speakers from segments
            total_speakers = 0
            confident_speakers = 0
            if 'segments' in result:
                speakers_found = set()
                confident_speakers_found = set()
                for segment in result['segments']:
                    if 'speaker' in segment:
                        speakers_found.add(segment['speaker'])
                        if segment.get('speaker_confidence', 0) >= speaker_confidence_threshold:
                            confident_speakers_found.add(segment['speaker'])
                
                total_speakers = len(speakers_found)
                confident_speakers = len(confident_speakers_found)
            
            # Apply timestamp granularity filtering
            granularities = [g.strip().lower() for g in timestamp_granularities.split(',')]
            
            # Convert numpy types to JSON-serializable types first
            result = convert_numpy_types(result)
            
            # Create the base response structure
            filtered_result = {}
            
            # Handle timestamp granularities filtering with improved logic
            logger.info(f"Applying timestamp granularities filter: {granularities}")
            
            if 'segment' in granularities and 'word' in granularities:
                # Both segment and word level - keep everything
                filtered_result = result.copy()
                logger.info("Keeping both segment and word level timestamps")
            elif 'segment' in granularities and 'word' not in granularities:
                # Only segment level - remove ALL word-level data
                filtered_result = result.copy()
                logger.info("Filtering to segment-only timestamps - removing word data")
                
                # Remove root level word arrays
                filtered_result.pop('words', None)
                filtered_result.pop('word_segments', None)  # In case this exists
                
                # Remove word-level data from segments
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        # Remove word-level timestamps but keep segment-level speaker info
                        segment.pop('words', None)
                        # Also remove any other word-level fields that might exist
                        segment.pop('word_segments', None)
                        
                logger.info(f"Segments after word removal: {len(filtered_result.get('segments', []))}")
                
            elif 'word' in granularities and 'segment' not in granularities:
                # Only word level - remove segments completely, keep only word data
                filtered_result = result.copy()
                logger.info("Filtering to word-only timestamps - removing segments")
                
                # Collect all words from segments before removing segments
                all_words = []
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        if 'words' in segment:
                            all_words.extend(segment['words'])
                
                # If no root-level words exist but we have segment words, move them to root
                if 'words' not in filtered_result and all_words:
                    filtered_result['words'] = all_words
                
                # Completely remove segments structure
                filtered_result.pop('segments', None)
                
                # Keep only word-level data at root level
                logger.info(f"Word-only result structure: root_words={'words' in filtered_result}, total_words={len(filtered_result.get('words', []))}, segments_removed=True")
                        
            else:
                # Default: keep segments if available, remove words to match default behavior
                filtered_result = result.copy()
                logger.info("Using default granularity (segment-only)")
                # Default to segment-only behavior
                filtered_result.pop('words', None)
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        segment.pop('words', None)
            
            # Handle diarization consistency
            if not enable_diarization:
                # Remove all speaker-related information
                if 'segments' in filtered_result:
                    for segment in filtered_result['segments']:
                        segment.pop('speaker', None)
                        segment.pop('speaker_confidence', None)
                        if 'words' in segment:
                            for word in segment['words']:
                                word.pop('speaker', None)
                                word.pop('speaker_confidence', None)
                if 'words' in filtered_result:
                    for word in filtered_result['words']:
                        word.pop('speaker', None)
                        word.pop('speaker_confidence', None)
            
            # Build processing info
            processing_info = {
                "device": self.device,
                "language_detected": filtered_result.get("language", "en"),
                "audio_duration": getattr(filtered_result, "duration", 0),
                "total_speakers": total_speakers,
                "confident_speakers": confident_speakers,
                "timestamp_granularities": granularities,
                "diarization_enabled": enable_diarization
            }
            
            # Generate text-only content for use in various output formats
            text_content = ""
            if 'segments' in filtered_result:
                text_content = "\n".join([segment.get('text', '') for segment in filtered_result.get('segments', [])])
            elif 'words' in filtered_result:
                # Extract text from word-level data when segments are not available
                words_text = ' '.join([word.get('word', word.get('text', '')) for word in filtered_result.get('words', [])])
                text_content = words_text
            elif 'text' in filtered_result:
                text_content = filtered_result['text']
            
            # Filter response based on output_content
            if output_content == "text_only":
                # Return only the transcribed text, no timestamps
                return {"text": text_content.strip()}
            
            elif output_content == "timestamps_only":
                # Return timestamped segments with text (respecting granularity settings)
                logger.info(f"Processing timestamps_only output with granularities: {granularities}")
                
                # Return the filtered result with timestamps AND text content
                return {
                    "status": "success", 
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
            elif output_content == "both":
                # Return separate text-only section AND timestamped results
                return {
                    "status": "success",
                    "text": text_content.strip(),
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
            elif output_content == "metadata_only":
                # Return only processing info and metadata
                speakers_list = []
                if enable_diarization and 'segments' in filtered_result:
                    speakers_list = list(set(segment.get('speaker', 'SPEAKER_00') 
                                           for segment in filtered_result.get('segments', []) 
                                           if 'speaker' in segment))
                
                return {
                    "status": "success",
                    "processing_info": processing_info,
                    "speakers": speakers_list
                }
            
            else:
                # Default fallback to 'both' format
                return {
                    "status": "success",
                    "text": text_content.strip(),
                    "result": filtered_result,
                    "processing_info": processing_info
                }
            
        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

    def optimized_diarization_postprocessing(self, result: dict, diarization_result, speaker_confidence_threshold: float, min_speaker_duration: float, speaker_smoothing_enabled: bool, min_switch_duration: float, vad_validation_enabled: bool, audio_path: str = None, cached_waveform_data=None) -> dict:
        """
        OPTIMIZATION: Combined post-processing pipeline for better performance
        Combines speaker assignment, filtering, and smoothing in optimized sequence
        """
        try:
            logger.info("Starting optimized diarization post-processing pipeline...")
            
            # Step 1: Manual speaker assignment with confidence scoring
            result = self.manual_speaker_assignment(result, diarization_result, speaker_confidence_threshold)
            
            # Step 2: Combined spurious speaker filtering and smoothing
            # (More efficient than separate passes)
            result = self.combined_speaker_filtering_and_smoothing(
                result, min_speaker_duration, speaker_confidence_threshold,
                speaker_smoothing_enabled, min_switch_duration
            )
            
            # Step 3: VAD validation (only if enabled and we have cached data)
            if vad_validation_enabled:
                result = self.validate_speakers_with_vad(result, audio_path, cached_waveform_data)
            
            # Step 4: Hierarchical clustering refinement
            result = self.apply_hierarchical_clustering(result)
            
            logger.info("âœ… Optimized post-processing pipeline complete")
            return result
            
        except Exception as e:
            logger.error(f"Error in optimized post-processing: {e}")
            return result

    def combined_speaker_filtering_and_smoothing(self, result: dict, min_speaker_duration: float, speaker_confidence_threshold: float, speaker_smoothing_enabled: bool, min_switch_duration: float) -> dict:
        """
        OPTIMIZATION: Combined filtering and smoothing in single pass
        More efficient than separate operations
        """
        try:
            segments = result.get('segments', [])
            if len(segments) < 2:
                return result
                
            logger.info("Applying combined speaker filtering and smoothing...")
            
            # First pass: Calculate speaker statistics for filtering
            # Handle both word-level and segment-level modes
            speaker_stats = {}
            for segment in segments:
                if 'words' in segment:
                    # Word-level mode
                    for word in segment['words']:
                        speaker = word.get('speaker', 'SPEAKER_00')
                        confidence = word.get('speaker_confidence', 0.5)
                        duration = word.get('end', 0) - word.get('start', 0)
                        
                        if speaker not in speaker_stats:
                            speaker_stats[speaker] = {
                                'total_duration': 0,
                                'confidences': [],
                                'word_count': 0
                            }
                        
                        speaker_stats[speaker]['total_duration'] += duration
                        speaker_stats[speaker]['confidences'].append(confidence)
                        speaker_stats[speaker]['word_count'] += 1
                else:
                    # Segment-level mode
                    speaker = segment.get('speaker', 'SPEAKER_00')
                    confidence = segment.get('speaker_confidence', 0.5)
                    duration = segment.get('end', 0) - segment.get('start', 0)
                    
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = {
                            'total_duration': 0,
                            'confidences': [],
                            'word_count': 0
                        }
                    
                    speaker_stats[speaker]['total_duration'] += duration
                    speaker_stats[speaker]['confidences'].append(confidence)
                    speaker_stats[speaker]['word_count'] += 1
            
            # Calculate average confidence per speaker
            for speaker in speaker_stats:
                confidences = speaker_stats[speaker]['confidences']
                speaker_stats[speaker]['avg_confidence'] = sum(confidences) / len(confidences)
            
            # Identify spurious speakers with adaptive filtering
            spurious_speakers = self.identify_spurious_speakers(speaker_stats, min_speaker_duration, speaker_confidence_threshold)
            
            # Find dominant speaker for reassignment
            dominant_speaker = self.find_dominant_speaker(speaker_stats, spurious_speakers)
            
            # Combined pass: Apply smoothing AND spurious speaker reassignment
            changes_made = 0
            reassignments_made = 0
            
            for i, segment in enumerate(segments):
                # Spurious speaker reassignment
                if segment.get('speaker') in spurious_speakers:
                    segment['speaker'] = dominant_speaker
                    segment['speaker_confidence'] = 0.6
                    if 'words' in segment:
                        for word in segment['words']:
                            if word.get('speaker') in spurious_speakers:
                                word['speaker'] = dominant_speaker
                                word['speaker_confidence'] = 0.6
                    reassignments_made += 1
                
                # Speaker smoothing (A â†’ B â†’ A pattern detection)
                if (speaker_smoothing_enabled and i > 0 and i < len(segments) - 1):
                    prev_segment = segments[i - 1]
                    next_segment = segments[i + 1]
                    
                    prev_speaker = prev_segment.get('speaker')
                    current_speaker = segment.get('speaker')
                    next_speaker = next_segment.get('speaker')
                    
                    # Check for A â†’ B â†’ A pattern
                    if (prev_speaker == next_speaker and 
                        current_speaker != prev_speaker and
                        prev_speaker is not None and current_speaker is not None):
                        
                        current_duration = segment.get('end', 0) - segment.get('start', 0)
                        
                        if current_duration < min_switch_duration:
                            # Check confidence levels
                            prev_conf = prev_segment.get('speaker_confidence', 0.5)
                            current_conf = segment.get('speaker_confidence', 0.5)
                            next_conf = next_segment.get('speaker_confidence', 0.5)
                            
                            avg_surrounding_conf = (prev_conf + next_conf) / 2
                            if avg_surrounding_conf > current_conf:
                                # Apply smoothing
                                segment['speaker'] = prev_speaker
                                segment['speaker_confidence'] = avg_surrounding_conf
                                
                                if 'words' in segment:
                                    for word in segment['words']:
                                        word['speaker'] = prev_speaker
                                        word['speaker_confidence'] = avg_surrounding_conf
                                
                                changes_made += 1
            
            if spurious_speakers:
                logger.info(f"âœ… Removed {len(spurious_speakers)} spurious speakers, reassigned {reassignments_made} segments")
            
            if changes_made > 0:
                logger.info(f"âœ… Applied speaker smoothing to {changes_made} segments")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in combined filtering and smoothing: {e}")
            return result

    def identify_spurious_speakers(self, speaker_stats: dict, min_speaker_duration: float, speaker_confidence_threshold: float) -> list:
        """Helper method to identify spurious speakers with adaptive thresholds"""
        spurious_speakers = []
        total_speakers = len(speaker_stats)
        
        # Adaptive filtering based on speaker count
        if total_speakers <= 3:
            duration_multiplier = 2.0
            confidence_multiplier = 1.2
            word_multiplier = 1.5
        else:
            duration_multiplier = 0.8
            confidence_multiplier = 0.9
            word_multiplier = 0.8
        
        for speaker, stats in speaker_stats.items():
            is_spurious = False
            
            # Duration check
            adaptive_min_duration = min_speaker_duration * duration_multiplier
            if stats['total_duration'] < adaptive_min_duration:
                is_spurious = True
            
            # Confidence check
            all_confidences = [stats['avg_confidence'] for stats in speaker_stats.values()]
            if len(all_confidences) > 1:
                base_threshold = min(speaker_confidence_threshold, max(all_confidences) * 0.75)
                confidence_threshold = base_threshold * confidence_multiplier
            else:
                confidence_threshold = speaker_confidence_threshold * 0.8 * confidence_multiplier
            
            if stats['avg_confidence'] < confidence_threshold:
                is_spurious = True
            
            # Word count check
            adaptive_min_words = max(5, adaptive_min_duration * 2 * word_multiplier)
            if stats['word_count'] < adaptive_min_words:
                is_spurious = True
            
            if is_spurious:
                spurious_speakers.append(speaker)
        
        return spurious_speakers

    def find_dominant_speaker(self, speaker_stats: dict, spurious_speakers: list) -> str:
        """Helper method to find the most reliable speaker for reassignment"""
        if not speaker_stats:
            return "SPEAKER_00"
        
        reliable_speakers = {
            spk: stats['avg_confidence'] * stats['total_duration']
            for spk, stats in speaker_stats.items()
            if spk not in spurious_speakers
        }
        
        if reliable_speakers:
            return max(reliable_speakers.items(), key=lambda x: x[1])[0]
        else:
            return "SPEAKER_00"


# FastAPI app
app = FastAPI(title="WhisperX Diarization Server with Multi-Cloud Storage")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global server instance
server = WhisperXDiarizationServer()

# Job tracking and status management
active_jobs = {}  # Track active jobs: {job_id: {"status": "processing"|"completed"|"failed", "start_time": timestamp, ...}}
job_lock = asyncio.Lock()  # Lock for thread-safe job dictionary access

# Helper functions for local job result storage
def get_job_result_path(job_id: str) -> str:
    """Get the file path for a job result"""
    return os.path.join(JOB_RESULTS_DIR, f"{job_id}.json")

def save_job_result_local(job_id: str, result: dict) -> None:
    """Save job result to local filesystem"""
    if JOB_RESULT_RETENTION_SECONDS <= 0:
        return
    
    result_path = get_job_result_path(job_id)
    try:
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        logger.debug(f"ðŸ’¾ Saved job result locally: {result_path}")
    except Exception as e:
        logger.error(f"âŒ Failed to save job result locally: {e}")

def load_job_result_local(job_id: str) -> Optional[dict]:
    """Load job result from local filesystem"""
    if JOB_RESULT_RETENTION_SECONDS <= 0:
        return None
    
    result_path = get_job_result_path(job_id)
    if not os.path.exists(result_path):
        return None
    
    try:
        with open(result_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"âŒ Failed to load job result from {result_path}: {e}")
        return None

def cleanup_old_job_results() -> int:
    """
    Clean up job result files older than JOB_RESULT_RETENTION_SECONDS
    Returns number of files cleaned up
    """
    if JOB_RESULT_RETENTION_SECONDS <= 0:
        return 0
    
    if not os.path.exists(JOB_RESULTS_DIR):
        return 0
    
    current_time = time.time()
    cleanup_count = 0
    
    try:
        for filename in os.listdir(JOB_RESULTS_DIR):
            if not filename.endswith('.json'):
                continue
            
            file_path = os.path.join(JOB_RESULTS_DIR, filename)
            try:
                # Check file age based on modification time
                file_mtime = os.path.getmtime(file_path)
                if current_time - file_mtime > JOB_RESULT_RETENTION_SECONDS:
                    os.unlink(file_path)
                    cleanup_count += 1
                    logger.debug(f"ðŸ—‘ï¸ Cleaned up old job result: {filename}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to cleanup {filename}: {e}")
        
        if cleanup_count > 0:
            logger.info(f"ðŸ—‘ï¸ Cleaned up {cleanup_count} old job result(s)")
    except Exception as e:
        logger.error(f"âŒ Error during job result cleanup: {e}")
    
    return cleanup_count

# Storage utility functions
async def download_from_r2(bucket: str, key: str) -> str:
    """Download file from R2 to temporary file (ASYNC - non-blocking)"""
    if not r2_client:
        raise HTTPException(500, "R2 client not configured")
    
    try:
        temp_file_path = tempfile.mktemp(suffix='.audio')
        
        # Run boto3 download in thread pool to avoid blocking event loop
        await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: r2_client.download_file(bucket, key, temp_file_path)
        )
        logger.info(f"Downloaded R2 object: s3://{bucket}/{key} -> {temp_file_path}")
        return temp_file_path
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'NoSuchKey':
            raise HTTPException(404, f"File not found in R2: {bucket}/{key}")
        elif error_code == 'NoSuchBucket':
            raise HTTPException(404, f"Bucket not found in R2: {bucket}")
        else:
            raise HTTPException(500, f"R2 download failed: {str(e)}")
    except Exception as e:
        raise HTTPException(500, f"Failed to download from R2: {str(e)}")

async def download_from_s3_presigned(presigned_url: str) -> str:
    """Download file from pre-signed S3 URL to temporary file (ASYNC - non-blocking)"""
    try:
        # Use httpx async client for truly non-blocking downloads
        async with httpx.AsyncClient(timeout=httpx.Timeout(300.0, connect=10.0)) as client:
            async with client.stream('GET', presigned_url) as response:
                response.raise_for_status()
                
                with tempfile.NamedTemporaryFile(delete=False, suffix='.audio') as temp_file:
                    temp_file_path = temp_file.name
                    
                # Async stream download to avoid memory issues AND event loop blocking
                with open(temp_file_path, 'wb') as f:
                    async for chunk in response.aiter_bytes(chunk_size=8192):
                        f.write(chunk)
                        # Yield control to event loop periodically
                        await asyncio.sleep(0)
                    
        logger.info(f"âœ… Downloaded from pre-signed S3 URL -> {temp_file_path}")
        return temp_file_path
        
    except httpx.HTTPStatusError as e:
        raise HTTPException(400, f"Failed to download from pre-signed URL: HTTP {e.response.status_code}")
    except httpx.RequestError as e:
        raise HTTPException(400, f"Failed to download from pre-signed URL: {str(e)}")
    except Exception as e:
        raise HTTPException(500, f"Unexpected error downloading from S3: {str(e)}")

async def upload_result_to_storage(result: dict, bucket: str, key: str, use_r2: bool = False) -> str:
    """Upload transcription result to R2 or S3 bucket"""
    client = r2_client if use_r2 else s3_client
    
    if not client:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"{service} client not configured")
    
    try:
        # Convert result to JSON and upload
        result_json = json.dumps(result, indent=2, ensure_ascii=False)
        
        client.put_object(
            Bucket=bucket,
            Key=key,
            Body=result_json.encode('utf-8'),
            ContentType='application/json',
            Metadata={
                'uploaded_by': 'whisper-server',
                'timestamp': datetime.now().isoformat(),
                'content_type': 'transcription_result'
            }
        )
        
        service = "R2" if use_r2 else "S3"
        storage_url = f"s3://{bucket}/{key}"
        logger.info(f"Uploaded result to {service}: {storage_url}")
        return storage_url
        
    except ClientError as e:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"Failed to upload result to {service}: {str(e)}")
    except Exception as e:
        service = "R2" if use_r2 else "S3"
        raise HTTPException(500, f"Unexpected error uploading to {service}: {str(e)}")

async def upload_to_user_presigned_url(result: dict, presigned_url: str, max_retries: int = 3) -> str:
    """
    Upload transcription result to user-provided pre-signed URL with retry logic
    
    Features:
    - Retry logic with exponential backoff
    - Configurable timeouts (UPLOAD_TIMEOUT_SECONDS)
    - Unicode escape decoding (\u0026 -> &)
    - Smart header handling (only signed headers)
    - Granular timeout control
    - Detailed error logging
    - 4xx vs 5xx error handling (don't retry 4xx)
    """
    import httpx
    import zlib
    import base64
    from urllib.parse import parse_qs, urlparse
    
    # Decode any JSON-escaped characters in the URL (e.g., \u0026 -> &)
    if '\\u' in presigned_url:
        try:
            presigned_url = presigned_url.encode('utf-8').decode('unicode_escape')
            logger.info("Decoded unicode escapes in presigned URL")
        except Exception as e:
            logger.warning(f"Failed to decode URL escapes: {e}")
    
    # Convert result to JSON
    result_json = json.dumps(result, indent=2, ensure_ascii=False)
    body = result_json.encode('utf-8')
    
    # Parse URL to check for checksum requirements and signed headers
    parsed = urlparse(presigned_url)
    query_params = parse_qs(parsed.query)
    
    # Check which headers were signed (to avoid signature mismatch)
    signed_headers_param = query_params.get('X-Amz-SignedHeaders', query_params.get('x-amz-signedheaders', ['']))
    signed_headers = signed_headers_param[0].split(';') if signed_headers_param else []
    
    # CRITICAL: Only send headers that were actually signed in the presigned URL
    # AWS will reject the request if we add ANY headers that weren't signed
    # 
    # HOWEVER: Content-Type is special - if not signed, AWS will use binary/octet-stream
    # We MUST add it to get proper JSON content type, even if not in signed headers
    headers = {'Content-Type': 'application/json'}
    
    # Log whether Content-Type was in signed headers
    if 'content-type' in signed_headers:
        logger.info("âœ… Content-Type was signed in URL - explicitly setting to application/json")
    else:
        logger.info("âš ï¸  Content-Type NOT signed in URL - adding anyway to prevent binary/octet-stream")
    
    # DO NOT add checksum headers - they are already in the URL query params if needed
    # Adding them as headers when they weren't signed will cause AccessDenied error
    
    # Retry logic with exponential backoff
    last_exception = None
    for attempt in range(max_retries):
        try:
            # Granular timeout control: connect=30s, read=UPLOAD_TIMEOUT_SECONDS, write=UPLOAD_TIMEOUT_SECONDS, pool=10s
            timeout_config = httpx.Timeout(
                connect=30.0,
                read=UPLOAD_TIMEOUT_SECONDS,
                write=UPLOAD_TIMEOUT_SECONDS,
                pool=10.0
            )
            
            logger.info(f"ðŸ“¤ Upload attempt {attempt + 1}/{max_retries} to user-provided URL (timeout: {UPLOAD_TIMEOUT_SECONDS}s)")
            
            # Upload using the pre-signed URL
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                response = await client.put(
                    presigned_url,
                    content=body,
                    headers=headers
                )
                
                if response.status_code in [200, 201]:
                    logger.info(f"âœ… Successfully uploaded result to user-provided URL (attempt {attempt + 1})")
                    return presigned_url
                elif response.status_code == 403:
                    response_text = response.text
                    # Check for specific AWS error conditions
                    if 'Request has expired' in response_text or 'expired' in response_text.lower():
                        # Presigned URL has expired - don't retry, this is a permanent error
                        logger.error(f"âŒ PRESIGNED URL EXPIRED: The presigned URL has expired. Job waited too long in queue or URL TTL was too short.")
                        logger.error(f"   Response: {response_text[:500]}")
                        raise HTTPException(500, "Presigned URL expired - URL TTL must be longer than max queue wait time + processing time")
                    elif 'SignatureDoesNotMatch' in response_text:
                        # Signature error - could be transient or URL issue, worth retrying
                        logger.warning(f"âš ï¸  SignatureDoesNotMatch error on attempt {attempt + 1}/{max_retries}")
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            logger.info(f"Waiting {wait_time}s before retry...")
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"âŒ SignatureDoesNotMatch persists after {max_retries} attempts: {response_text[:500]}")
                            raise HTTPException(500, f"AWS signature error after {max_retries} retries - check presigned URL generation")
                    else:
                        # Other 403 errors
                        logger.error(f"âŒ Access denied (403): {response_text[:500]}")
                        raise HTTPException(500, f"Access denied to presigned URL: {response_text[:200]}")
                elif 400 <= response.status_code < 500:
                    # Other client errors (4xx) - don't retry, fail immediately
                    logger.error(f"âŒ Client error uploading to user URL (HTTP {response.status_code}): {response.text[:500]}")
                    raise HTTPException(500, f"Failed to upload to user URL: HTTP {response.status_code} - {response.text[:200]}")
                else:
                    # Server error (5xx) - retry
                    logger.warning(f"âš ï¸ Server error uploading to user URL (HTTP {response.status_code}), will retry: {response.text[:500]}")
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt
                        logger.info(f"Waiting {wait_time}s before retry...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        raise HTTPException(500, f"Server error persists: HTTP {response.status_code}")
                    
        except httpx.ConnectTimeout as e:
            last_exception = e
            logger.warning(f"âš ï¸ Connect timeout on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
            
        except httpx.ReadTimeout as e:
            last_exception = e
            logger.warning(f"âš ï¸ Read timeout on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
            
        except httpx.WriteTimeout as e:
            last_exception = e
            logger.warning(f"âš ï¸ Write timeout on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
            
        except httpx.PoolTimeout as e:
            last_exception = e
            logger.warning(f"âš ï¸ Pool timeout on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
            
        except httpx.RequestError as e:
            last_exception = e
            logger.warning(f"âš ï¸ Network error on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
            
        except Exception as e:
            last_exception = e
            logger.error(f"âŒ Unexpected error uploading to user URL on attempt {attempt + 1}/{max_retries}: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
            continue
    
    # All retries failed
    error_msg = f"Failed to upload after {max_retries} attempts. Last error: {type(last_exception).__name__} - {str(last_exception)}"
    logger.error(f"âŒ {error_msg}")
    raise HTTPException(500, error_msg)

async def _background_task_wrapper(coro):
    """
    Wrapper to ensure background task exceptions are properly logged
    and don't silently fail
    """
    try:
        await coro
    except Exception as e:
        logger.error(f"âŒ CRITICAL: Background task failed with unhandled exception: {type(e).__name__} - {str(e)}")
        logger.error(traceback.format_exc())

async def process_transcription_background(
    job_id: str,
    temp_audio_path: Optional[str],
    upload_presigned_url: Optional[str],
    enable_diarization: bool,
    language: str,
    batch_size: int,
    prompt: str,
    timestamp_granularities: str,
    output_content: str,
    clustering_threshold: float,
    segmentation_threshold: float,
    min_speaker_duration: float,
    speaker_confidence_threshold: float,
    speaker_smoothing_enabled: bool,
    min_switch_duration: float,
    vad_validation_enabled: bool,
    optimized_alignment: bool,
    s3_presigned_url: Optional[str] = None,
    storage_key: Optional[str] = None
):
    """
    Background task to process transcription and upload result to user's S3 bucket
    Handles downloading audio from s3_presigned_url or storage_key if provided
    
    Enhanced with job status tracking:
    - Updates active_jobs dictionary with status/progress
    - Structured processing: Download â†’ Process â†’ Upload
    - Progress updates: "Downloading", "Transcribing", "Uploading", "Done"
    - Error uploads: Failed jobs still upload error details to user URL
    - Automatic cleanup: Removes completed/failed job records older than 12 hours
    - Safe cleanup: Never removes current job or jobs still processing
    - Presigned URL expiration detection with clear error messages
    """
    start_time = time.time()
    
    try:
        # Initialize job tracking
        async with job_lock:
            active_jobs[job_id] = {
                "status": "processing",
                "progress": "Initializing",
                "start_time": start_time,
                "timeout_at": start_time + JOB_PROCESSING_TIMEOUT_SECONDS if JOB_PROCESSING_TIMEOUT_SECONDS > 0 else None,
                "input_source": "file" if temp_audio_path else ("s3_url" if s3_presigned_url else "storage_key")
            }
        
        logger.info(f"ðŸ”„ Starting background transcription for job {job_id}")
        
        # STEP 1: Download audio if needed (outside GPU - I/O bound)
        if not temp_audio_path:
            # Check timeout
            if JOB_PROCESSING_TIMEOUT_SECONDS > 0 and time.time() - start_time > JOB_PROCESSING_TIMEOUT_SECONDS:
                raise TimeoutError(f"Job exceeded maximum processing time of {JOB_PROCESSING_TIMEOUT_SECONDS}s")
            
            async with job_lock:
                active_jobs[job_id]["progress"] = "Downloading audio"
                active_jobs[job_id]["status"] = "processing"
            
            if s3_presigned_url:
                logger.info(f"ðŸ“¥ Downloading audio from pre-signed URL for job {job_id}...")
                temp_audio_path = await download_from_s3_presigned(s3_presigned_url)
            elif storage_key:
                logger.info(f"ðŸ“¥ Downloading audio from storage for job {job_id}...")
                bucket = DEFAULT_UPLOAD_BUCKET
                temp_audio_path = await download_from_r2(bucket, storage_key)
        
        # STEP 2: Process transcription with GPU
        # Check timeout before starting GPU work
        if JOB_PROCESSING_TIMEOUT_SECONDS > 0 and time.time() - start_time > JOB_PROCESSING_TIMEOUT_SECONDS:
            raise TimeoutError(f"Job exceeded maximum processing time of {JOB_PROCESSING_TIMEOUT_SECONDS}s")
        
        async with job_lock:
            active_jobs[job_id]["progress"] = "Transcribing audio"
            active_jobs[job_id]["status"] = "processing"
        
        logger.info(f"ðŸŽ™ï¸ Transcribing audio for job {job_id}...")
        # Run GPU transcription in thread pool to allow concurrent GPU processing
        # This offloads blocking GPU calls from event loop, enabling multiple jobs to process simultaneously
        # 
        # IMPORTANT: output_content parameter controls what gets uploaded to AWS
        # The inline API response is ALWAYS metadata-only for async mode (handled at endpoint level)
        # But the AWS upload respects user's output_content choice
        result = await asyncio.get_event_loop().run_in_executor(
            gpu_executor,
            lambda: server.transcribe_with_diarization(
                temp_audio_path, 
                enable_diarization=enable_diarization,
                language=language,
                model="large-v3-turbo",
                batch_size=batch_size,
                prompt=prompt,
                temperature=0.0,
                timestamp_granularities=timestamp_granularities,
                output_content=output_content,  # User's choice - controls AWS upload content
                clustering_threshold=clustering_threshold,
                segmentation_threshold=segmentation_threshold,
                min_speaker_duration=min_speaker_duration,
                speaker_confidence_threshold=speaker_confidence_threshold,
                speaker_smoothing_enabled=speaker_smoothing_enabled,
                min_switch_duration=min_switch_duration,
                vad_validation_enabled=vad_validation_enabled,
                optimized_alignment=optimized_alignment
            )
        )
        
        # Add job ID and timing to result
        if isinstance(result, dict):
            result["job_id"] = job_id
            result["status"] = "completed"
            result["processing_time"] = time.time() - start_time
        
        # STEP 3: Upload result (outside GPU - I/O bound)
        # Check timeout before uploading
        if JOB_PROCESSING_TIMEOUT_SECONDS > 0 and time.time() - start_time > JOB_PROCESSING_TIMEOUT_SECONDS:
            raise TimeoutError(f"Job exceeded maximum processing time of {JOB_PROCESSING_TIMEOUT_SECONDS}s")
        
        async with job_lock:
            if job_id in active_jobs:
                active_jobs[job_id]["progress"] = "Uploading results"
                active_jobs[job_id]["status"] = "processing"
            else:
                logger.warning(f"âš ï¸ Job {job_id} was removed from active_jobs before upload phase")
        
        logger.info(f"ðŸ“¤ Uploading results for job {job_id}...")
        
        # ALWAYS store result locally for retrieval via GET endpoint (if retention is enabled)
        if JOB_RESULT_RETENTION_SECONDS > 0:
            try:
                save_job_result_local(job_id, result)
                logger.info(f"âœ… Stored result locally for GET retrieval")
            except Exception as storage_error:
                logger.error(f"âš ï¸ Failed to store result locally (GET endpoint won't work): {storage_error}")
                # Don't fail the job if local storage fails - continue with callback upload
        
        # Upload to user's presigned URL if provided (optional callback)
        if upload_presigned_url:
            await upload_to_user_presigned_url(result, upload_presigned_url)
        else:
            logger.info(f"âœ… No callback URL provided, results stored locally only")
        
        # Mark job as completed
        end_time = time.time()
        async with job_lock:
            if job_id in active_jobs:
                active_jobs[job_id]["status"] = "completed"
                active_jobs[job_id]["progress"] = "Done"
                active_jobs[job_id]["end_time"] = end_time
                active_jobs[job_id]["elapsed_time"] = end_time - start_time
                if JOB_RESULT_RETENTION_SECONDS > 0:
                    active_jobs[job_id]["result_available"] = True
                    active_jobs[job_id]["result_expires_at"] = end_time + JOB_RESULT_RETENTION_SECONDS
            else:
                logger.warning(f"âš ï¸ Job {job_id} was removed from active_jobs before marking complete")
        
        logger.info(f"âœ… Background job {job_id} completed successfully in {end_time - start_time:.2f}s")
        
    except TimeoutError as e:
        end_time = time.time()
        logger.error(f"â±ï¸ Background job {job_id} TIMEOUT after {end_time - start_time:.2f}s: {e}")
        
        # Create timeout error result
        error_result = {
            "job_id": job_id,
            "status": "failed",
            "error": f"Job exceeded maximum processing time of {JOB_PROCESSING_TIMEOUT_SECONDS}s ({JOB_PROCESSING_TIMEOUT_SECONDS/3600:.1f} hours)",
            "error_type": "TimeoutError",
            "processing_time": end_time - start_time,
            "timeout_limit": JOB_PROCESSING_TIMEOUT_SECONDS
        }
        
        # Store timeout error result locally for GET retrieval (if retention is enabled)
        if JOB_RESULT_RETENTION_SECONDS > 0:
            try:
                save_job_result_local(job_id, error_result)
                logger.info(f"âœ… Stored timeout error result locally for GET retrieval")
            except Exception as storage_error:
                logger.error(f"âš ï¸ Failed to store timeout error result locally: {storage_error}")
        
        # Update job status to failed
        async with job_lock:
            if job_id in active_jobs:
                active_jobs[job_id]["status"] = "failed"
                active_jobs[job_id]["progress"] = "Timeout"
                active_jobs[job_id]["end_time"] = end_time
                active_jobs[job_id]["elapsed_time"] = end_time - start_time
                active_jobs[job_id]["error"] = error_result["error"]
                active_jobs[job_id]["error_type"] = "TimeoutError"
                if JOB_RESULT_RETENTION_SECONDS > 0:
                    active_jobs[job_id]["result_available"] = True
                    active_jobs[job_id]["result_expires_at"] = end_time + JOB_RESULT_RETENTION_SECONDS
            else:
                logger.warning(f"âš ï¸ Job {job_id} was removed from active_jobs before marking timeout")
        
        # Try to upload timeout error result to user's URL if provided
        if upload_presigned_url:
            try:
                await upload_to_user_presigned_url(error_result, upload_presigned_url)
                logger.info(f"ðŸ“¤ Uploaded timeout error result for job {job_id}")
            except Exception as upload_error:
                logger.error(f"âŒ Failed to upload timeout error result for job {job_id}: {upload_error}")
    
    except Exception as e:
        end_time = time.time()
        logger.error(f"âŒ Background job {job_id} failed after {end_time - start_time:.2f}s: {e}")
        logger.error(traceback.format_exc())
        
        # Create error result
        error_result = {
            "job_id": job_id,
            "status": "failed",
            "error": str(e),
            "error_type": type(e).__name__,
            "processing_time": end_time - start_time
        }
        
        # Store error result locally for GET retrieval (if retention is enabled)
        if JOB_RESULT_RETENTION_SECONDS > 0:
            try:
                save_job_result_local(job_id, error_result)
                logger.info(f"âœ… Stored error result locally for GET retrieval")
            except Exception as storage_error:
                logger.error(f"âš ï¸ Failed to store error result locally: {storage_error}")
        
        # Update job status to failed
        async with job_lock:
            if job_id in active_jobs:
                active_jobs[job_id]["status"] = "failed"
                active_jobs[job_id]["progress"] = "Failed"
                active_jobs[job_id]["end_time"] = end_time
                active_jobs[job_id]["elapsed_time"] = end_time - start_time
                active_jobs[job_id]["error"] = str(e)
                active_jobs[job_id]["error_type"] = type(e).__name__
                if JOB_RESULT_RETENTION_SECONDS > 0:
                    active_jobs[job_id]["result_available"] = True
                    active_jobs[job_id]["result_expires_at"] = end_time + JOB_RESULT_RETENTION_SECONDS
            else:
                logger.warning(f"âš ï¸ Job {job_id} was removed from active_jobs before marking failed")
        
        # Try to upload error result to user's URL if provided
        if upload_presigned_url:
            try:
                await upload_to_user_presigned_url(error_result, upload_presigned_url)
                logger.info(f"ðŸ“¤ Uploaded error result for job {job_id}")
            except Exception as upload_error:
                logger.error(f"âŒ Failed to upload error result for job {job_id}: {upload_error}")
    
    finally:
        # Cleanup temporary files
        if temp_audio_path and os.path.exists(temp_audio_path):
            try:
                os.unlink(temp_audio_path)
                logger.debug(f"ðŸ—‘ï¸ Cleaned up temporary audio file for job {job_id}: {temp_audio_path}")
            except Exception as cleanup_error:
                logger.warning(f"âš ï¸ Failed to cleanup temporary file {temp_audio_path}: {cleanup_error}")
        
        # Cleanup old job records and result files
        # CRITICAL: Don't remove jobs that are still processing or just completed
        try:
            async with job_lock:
                current_time = time.time()
                # Use configured retention period for cleanup
                retention_seconds = JOB_RESULT_RETENTION_SECONDS if JOB_RESULT_RETENTION_SECONDS > 0 else 43200
                
                # Only cleanup jobs that are completed/failed AND old
                jobs_to_remove = [
                    jid for jid, job_data in active_jobs.items()
                    if (
                        jid != job_id  # Never remove the current job
                        and job_data.get("status") in ["completed", "failed"]  # Only cleanup finished jobs
                        and current_time - job_data.get("start_time", current_time) > retention_seconds
                    )
                ]
                for jid in jobs_to_remove:
                    del active_jobs[jid]
                    logger.debug(f"ðŸ—‘ï¸ Removed old job record: {jid}")
                
                if jobs_to_remove:
                    logger.info(f"ðŸ—‘ï¸ Cleaned up {len(jobs_to_remove)} old job records")
            
            # Clean up old result files from filesystem
            cleanup_old_job_results()
        except Exception as cleanup_error:
            logger.warning(f"âš ï¸ Failed to cleanup old job records: {cleanup_error}")

async def periodic_cleanup_task():
    """Background task to periodically clean up old job results"""
    while True:
        try:
            await asyncio.sleep(3600)  # Run every hour
            logger.info("ðŸ§¹ Running periodic cleanup of old job results...")
            cleanup_count = cleanup_old_job_results()
            if cleanup_count > 0:
                logger.info(f"ðŸ§¹ Periodic cleanup removed {cleanup_count} old result(s)")
        except Exception as e:
            logger.error(f"âŒ Error in periodic cleanup task: {e}")

@app.on_event("startup")
async def startup_event():
    """Initialize models and storage clients on startup"""
    try:
        init_storage_clients()
        server.load_models()
        
        # Start periodic cleanup task
        if JOB_RESULT_RETENTION_SECONDS > 0:
            asyncio.create_task(periodic_cleanup_task())
            logger.info(f"ðŸ§¹ Started periodic cleanup task (retention: {JOB_RESULT_RETENTION_SECONDS}s)")
    except Exception as e:
        logger.error(f"Failed to load models on startup: {e}")

@app.get("/health")
async def health_check():
    """
    Health check endpoint with system status and concurrency metrics
    """
    # Count jobs by status
    async with job_lock:
        total_jobs = len(active_jobs)
        processing_jobs = sum(1 for job in active_jobs.values() if job.get("status") == "processing")
        completed_jobs = sum(1 for job in active_jobs.values() if job.get("status") == "completed")
        failed_jobs = sum(1 for job in active_jobs.values() if job.get("status") == "failed")
    
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "device": server.device,
        "concurrency": {
            "max_concurrent_requests": MAX_CONCURRENT_REQUESTS,
            "active_jobs": total_jobs,
            "processing": processing_jobs,
            "completed": completed_jobs,
            "failed": failed_jobs,
            "note": "Concurrency is handled by Python's async event loop - no artificial limits"
        },
        "timeouts": {
            "upload_timeout_seconds": UPLOAD_TIMEOUT_SECONDS,
            "upload_timeout_minutes": UPLOAD_TIMEOUT_SECONDS / 60.0,
            "note": "Timeout for uploading results to S3. Transcription itself has no timeout."
        },
        "models_loaded": {
            "transcription": server.transcription_model is not None,
            "alignment": server.alignment_model is not None,
            "diarization": server.diarization_pipeline is not None
        },
        "storage_clients": {
            "r2_enabled": r2_client is not None,
            "s3_enabled": s3_client is not None
        }
    }

@app.get("/v1/jobs/{job_id}")
async def get_job_status(job_id: str, include_result: bool = True):
    """
    Get status and results of a specific transcription job
    
    Returns job status, progress, timing information, and full transcription results if completed.
    Job records and results are automatically cleaned up based on JOB_RESULT_RETENTION_SECONDS.
    
    **Parameters:**
    - `job_id`: The unique job identifier returned when starting an async transcription
    - `include_result`: If true (default), includes full transcription results when job is completed
    
    **Returns:**
    - Job status and progress information
    - Timing details (start time, elapsed time, end time if completed)
    - Full transcription results (if job completed and include_result=true)
    - Error information if the job failed
    
    **Response Example (Processing):**
    ```json
    {
      "job_id": "abc123...",
      "status": "processing",
      "progress": "Transcribing audio",
      "start_time": 1234567890.123,
      "elapsed_time": 45.2
    }
    ```
    
    **Response Example (Completed):**
    ```json
    {
      "job_id": "abc123...",
      "status": "completed",
      "progress": "Done",
      "start_time": 1234567890.123,
      "elapsed_time": 120.5,
      "end_time": 1234568010.623,
      "result": {
        "text": "Full transcription...",
        "segments": [...],
        "language": "en"
      }
    }
    ```
    """
    async with job_lock:
        if job_id not in active_jobs:
            raise HTTPException(404, f"Job not found: {job_id}. Note: Job records are automatically cleaned up after completion based on retention settings.")
        
        job_data = active_jobs[job_id].copy()
    
    # Calculate elapsed time if job is still processing
    if job_data.get("status") == "processing" and "start_time" in job_data:
        job_data["elapsed_time"] = time.time() - job_data["start_time"]
    
    job_data["job_id"] = job_id
    
    # If job is completed/failed and result storage is enabled, try to load the full result
    if include_result and job_data.get("status") in ["completed", "failed"] and JOB_RESULT_RETENTION_SECONDS > 0:
        try:
            full_result = load_job_result_local(job_id)
            if full_result:
                # Merge the stored result with job metadata
                # Keep job tracking data, but add the full transcription result
                job_data["result"] = full_result
                logger.debug(f"âœ… Loaded full result for job {job_id}")
            else:
                job_data["result_note"] = "Result not available (may have been cleaned up or failed to store)"
        except Exception as e:
            logger.error(f"âŒ Error loading result for job {job_id}: {e}")
            job_data["result_note"] = f"Error loading result: {str(e)}"
    
    return job_data

@app.get("/v1/jobs")
async def list_jobs():
    """
    List all active and recent transcription jobs
    
    Returns a summary of all jobs currently tracked by the server.
    Useful for monitoring system load and debugging.
    
    **Returns:**
    - Total job count
    - Dictionary of all jobs with their status and timing information
    
    **Response Example:**
    ```json
    {
      "total_jobs": 5,
      "jobs": {
        "abc123": {
          "status": "completed",
          "progress": "Done",
          "elapsed_time": 120.5,
          ...
        },
        "def456": {
          "status": "processing",
          "progress": "Transcribing audio",
          "elapsed_time": 45.2,
          ...
        }
      }
    }
    ```
    """
    async with job_lock:
        jobs_copy = {}
        current_time = time.time()
        
        for job_id, job_data in active_jobs.items():
            job_info = job_data.copy()
            
            # Calculate elapsed time for processing jobs
            if job_info.get("status") == "processing" and "start_time" in job_info:
                job_info["elapsed_time"] = current_time - job_info["start_time"]
            
            jobs_copy[job_id] = job_info
    
    return {
        "total_jobs": len(jobs_copy),
        "jobs": jobs_copy
    }

@app.post("/v1/uploads")
async def upload_files(
    file: UploadFile = File(..., description="Audio file to upload"),
    key: Optional[str] = Form(None, description="Object key (auto-generated if not provided)")
):
    """
    Upload Files
    
    Upload audio files directly to cloud storage and receive the object reference 
    that can be used later for transcription.
    
    **Parameters:**
    - `file`: The audio file to upload  
    - `key`: Optional custom object key (auto-generated if not provided)
    
    **Returns:**
    - Storage object details including bucket, key, and file info
    """
    if not r2_client:
        raise HTTPException(500, "Cloud storage not configured. Check server storage credentials.")
    
    # Use environment-configured bucket
    bucket = DEFAULT_UPLOAD_BUCKET
    logger.info(f"Using configured upload bucket: {bucket}")
        
    # Generate key if not provided
    if not key:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        file_ext = os.path.splitext(file.filename or "audio")[1] or ".wav"
        key = f"uploads/{timestamp}{file_ext}"
    
    try:
        # Read file content
        file_content = await file.read()
        file_size = len(file_content)
        
        # Upload to storage
        r2_client.put_object(
            Bucket=bucket,
            Key=key,
            Body=file_content,
            ContentType=file.content_type or 'audio/wav',
            Metadata={
                'original_filename': file.filename or 'unknown',
                'upload_timestamp': datetime.now().isoformat(),
                'uploaded_by': 'whisper-server'
            }
        )
        
        logger.info(f"Successfully uploaded file to storage: {bucket}/{key} ({file_size} bytes)")
        
        return {
            "status": "success",
            "message": "File uploaded successfully",
            "bucket": bucket,
            "key": key,
            "file_size": file_size,
            "original_filename": file.filename,
            "content_type": file.content_type,
            "upload_timestamp": datetime.now().isoformat(),
            "storage_url": f"s3://{bucket}/{key}"
        }
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'NoSuchBucket':
            raise HTTPException(404, f"Storage bucket not found: {bucket}")
        elif error_code == 'AccessDenied':
            raise HTTPException(403, f"Access denied to storage bucket: {bucket}")
        else:
            raise HTTPException(500, f"Storage upload failed: {str(e)}")
    except Exception as e:
        logger.error(f"Upload to storage failed: {e}")
        raise HTTPException(500, f"Failed to upload file: {str(e)}")

@app.post("/v1/downloads")
async def download_from_storage(
    key: str = Form(..., description="Object key/path of the file to download (e.g., transcriptions/filename.json)")
):
    """
    Download From Storage
    
    Download transcription results and other files from cloud storage.
    
    **Parameters:**
    - `key`: Object key/path of the file to download
    
    **Returns:**
    - The requested file as a downloadable response
    """
    if not r2_client:
        raise HTTPException(500, "Cloud storage client not configured")
    
    # Use environment-configured bucket
    bucket = DEFAULT_RESULTS_BUCKET
    logger.info(f"Downloading from configured results bucket: {bucket}")
    
    try:
        # Get the object from storage
        response = r2_client.get_object(Bucket=bucket, Key=key)
        
        # Get file content and metadata
        file_content = response['Body'].read()
        content_type = response.get('ContentType', 'application/octet-stream')
        
        # Extract filename from key
        filename = key.split('/')[-1] if '/' in key else key
        
        return Response(
            content=file_content,
            media_type=content_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
        
    except Exception as e:
        logger.error(f"Failed to download file {bucket}/{key}: {e}")
        raise HTTPException(404, f"File not found: {str(e)}")

@app.post("/v1/audio/transcriptions")
async def create_transcription(
    request: Request,
    
    # === FILE INPUT SECTION ===
    file: Optional[UploadFile] = File(None, description="Audio file to upload and transcribe"),
    
    # === FILE REFERENCE OPTIONS ===
    storage_key: Optional[str] = Form(None, description="Storage object key for existing file"),
    s3_presigned_url: Optional[str] = Form(None, description="Pre-signed URL to download audio file from S3"),
    
    # === TRANSCRIPTION OUTPUT SECTION ===
    response_format: str = Form("json", description="Response format: json, verbose_json, text, srt, vtt"),
    output_content: str = Form("both", description="Controls response content - 'text_only': plain text, 'timestamps_only': JSON with timestamps, 'both': text + timestamps, 'metadata_only': processing info only"),
    stored_output: bool = Form(False, description="Enable result storage - True: Save to storage, False: API response only"),
    output_key: Optional[str] = Form(None, description="Custom storage object key (auto-generated if not provided)"),
    upload_presigned_url: Optional[str] = Form(None, description="Pre-signed upload URL to save results to your own S3 bucket (enables async processing - returns immediately with job status)"),
    
    # === TRANSCRIPTION SETTINGS SECTION ===
    language: str = Form("en", description="Language code (e.g., 'en', 'es', 'fr', 'de')"),
    enable_diarization: bool = Form(True, description="Enable speaker diarization"),
    timestamp_granularities: str = Form("segment", description="Timestamp granularity: 'segment' or 'word' (comma-separated for both)"),
    prompt: str = Form("", description="Optional text prompt to provide context or guide the transcription (e.g., proper names, technical terms)"),
    
    # === FINE-TUNING SETTINGS SECTION ===
    batch_size: int = Form(16, description="Batch size for processing (higher = faster but more memory)"),
    clustering_threshold: float = Form(0.7, description="Speaker clustering threshold (0.5-1.0, lower = more speakers)"),
    segmentation_threshold: float = Form(0.45, description="Voice activity detection threshold (0.1-0.9)"),
    min_speaker_duration: float = Form(3.0, description="Minimum speaking time per speaker (seconds)"),
    speaker_confidence_threshold: float = Form(0.6, description="Minimum confidence for speaker assignment (0.1-1.0)"),
    speaker_smoothing_enabled: bool = Form(True, description="Enable speaker transition smoothing"),
    min_switch_duration: float = Form(2.0, description="Minimum time between speaker switches (seconds)"),
    vad_validation_enabled: bool = Form(False, description="Enable Voice Activity Detection validation (experimental)"),
    optimized_alignment: bool = Form(True, description="Use WAV2VEC2 alignment model (True) or Whisper built-in alignment (False) - True provides best accuracy")
):
    """
    **Advanced Audio Transcription with Speaker Identification**
    
    Upload audio files and receive accurate transcriptions with speaker diarization,
    supporting multiple input sources and automatic result export.
    
    **ðŸ“ File Input Options (choose ONE):**
    - `file`: Direct file upload (optional - recommended for most use cases)
    - `storage_key`: Reference to existing file in configured storage bucket  
    - `s3_presigned_url`: Pre-signed URL for secure file access
    
    **Note**: Exactly one input source must be provided. The API will fail if none are provided or if multiple are provided.
    
    **ðŸ“„ Transcription Output:**
    - `response_format`: Choose output format (JSON, SRT, VTT, plain text)
    - `output_content`: Control what's included in response:
      - `text_only`: Returns only plain text transcription (no timestamps)
      - `timestamps_only`: Returns JSON with timestamps and segments (no separate text field)
      - `both`: Returns both plain text AND timestamped segments (default)
      - `metadata_only`: Returns only processing metadata (no transcription text)
    - `stored_output`: **Controls processing mode** (true = async with storage, false = sync with inline results)
    - `upload_presigned_url`: Optional callback URL to upload results to your S3 bucket
    - `output_key`: [Deprecated] No longer used (kept for backward compatibility)
    
    **âš¡ Processing Modes:**
    - **Async Mode** (when `stored_output=true`):
      - Returns immediately with `job_id` and metadata
      - Processes transcription in background
      - Results stored locally on server for 24 hours (configurable)
      - Retrieve results via `GET /v1/jobs/{job_id}`
      - Optional: Also uploads to `upload_presigned_url` if provided
    - **Sync Mode** (when `stored_output=false`):
      - Waits for transcription to complete
      - Returns full result inline in API response
      - Respects `response_format` and `output_content` parameters
      - No server-side storage
    
    **Retrieval Options:**
    - **Polling** (async mode): Submit job, get `job_id`, poll `GET /v1/jobs/{job_id}` for results
    - **Callback** (async mode with `upload_presigned_url`): Results uploaded to your S3, also retrievable via GET endpoint
    - **Inline** (sync mode): Immediate response with full transcription results
    
    **âš™ï¸ Transcription Settings:**
    - Multi-language support with automatic detection
    - Advanced speaker diarization with confidence scoring
    - Flexible timestamp granularity (segment, word, or both)
    - Context prompts for improved accuracy with technical terms
    
    **ðŸŽ›ï¸ Fine-Tuning Settings:**
    - Speaker recognition parameters for optimal accuracy
    - GPU batch processing optimization
    - Voice activity detection controls
    - Speaker transition smoothing options
    
    **ðŸš€ Performance Features:**
    - GPU-accelerated processing pipeline
    - Automatic result archiving to cloud storage
    - Enterprise-grade reliability and cleanup
    - Memory-efficient processing for large files
    """
    temp_audio_path = None
    try:
        # DEBUG: Track timing
        import time
        start_time = time.time()
        logger.info(f"ðŸš€ Request started at {start_time}")

        # Check for async mode FIRST before any processing
        # Async mode is triggered by stored_output=true (results stored locally for later retrieval)
        # This allows immediate response instead of waiting for downloads or form processing
        is_async_mode = stored_output

        # Handle legacy parameters if provided (model, temperature) - these are ignored but logged
        # Only process form data if NOT in async mode to avoid blocking the response
        ignored_params = []
        if not is_async_mode:
            form_data = await request.form()

            # Check for model parameter (ignored)
            if "model" in form_data:
                model_value = form_data.get("model")
                if model_value != "large-v3-turbo":
                    ignored_params.append(f"model={model_value} (using large-v3-turbo)")

            # Check for temperature parameter (ignored)
            if "temperature" in form_data:
                temperature_value = form_data.get("temperature")
                try:
                    temp_float = float(temperature_value)
                    if temp_float != 0.0:
                        ignored_params.append(f"temperature={temperature_value} (using 0.0)")
                except (ValueError, TypeError):
                    ignored_params.append(f"temperature={temperature_value} (invalid, using 0.0)")

        if ignored_params:
            logger.info(f"â„¹ï¸  Ignored parameters in this request: {', '.join(ignored_params)}")

        # Validate input parameters
        file_provided = file is not None and file.filename is not None and file.filename.strip() != ""
        input_sources = sum([
            file_provided,
            bool(storage_key),
            bool(s3_presigned_url)
        ])

        if input_sources == 0:
            raise HTTPException(400, "One input source required: file upload, storage_key, or s3_presigned_url")
        elif input_sources > 1:
            raise HTTPException(400, "Only one input source allowed at a time")

        # Handle different input sources
        if file:
            # Direct file upload - use async file I/O to avoid blocking event loop
            temp_audio_path = tempfile.mktemp(suffix=".wav")
            
            # Read file content asynchronously to avoid blocking
            content = await file.read()
            
            # Write to temp file in thread pool to avoid blocking event loop
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: Path(temp_audio_path).write_bytes(content)
            )

        elif storage_key:
            # Storage object download from configured bucket
            if is_async_mode:
                # ASYNC MODE: Don't download yet, let background task do it
                logger.info(f"Async mode: Will download from storage in background")
                temp_audio_path = None  # Background task will download
            else:
                # SYNC MODE: Download now
                bucket = DEFAULT_UPLOAD_BUCKET
                logger.info(f"Processing storage object: {bucket}/{storage_key}")
                temp_audio_path = await download_from_r2(bucket, storage_key)

        elif s3_presigned_url:
            # Pre-signed S3 URL - download NOW for sync mode, or LATER in background for async mode
            if is_async_mode:
                # ASYNC MODE: Don't download yet, let background task do it
                logger.info(f"Async mode: Will download from pre-signed URL in background")
                temp_audio_path = None  # Background task will download
            else:
                # SYNC MODE: Download now
                temp_audio_path = await download_from_s3_presigned(s3_presigned_url)

        # ASYNC PROCESSING MODE: If stored_output=true, return immediately with job_id
        # Results will be stored locally and retrievable via GET /v1/jobs/{job_id}
        if stored_output:
            # Generate unique job ID
            job_id = str(uuid.uuid4())

            logger.info(f"ðŸš€ Starting async transcription job {job_id}")
            logger.info(f"   Results will be stored locally for retrieval")
            if upload_presigned_url:
                logger.info(f"   Will also upload results to user-provided presigned URL")

            # Prepare timestamp granularities for response
            granularities = [g.strip().lower() for g in timestamp_granularities.split(',')]

            # Use asyncio.create_task for TRUE parallel GPU processing
            # Multiple transcription jobs WILL run concurrently on the GPU
            asyncio.create_task(
                process_transcription_background(
                    job_id=job_id,
                    temp_audio_path=temp_audio_path,  # Will be path if file uploaded, None if download needed
                    upload_presigned_url=upload_presigned_url,
                    enable_diarization=enable_diarization,
                    language=language,
                    batch_size=batch_size,
                    prompt=prompt,
                    timestamp_granularities=timestamp_granularities,
                    output_content=output_content,
                    clustering_threshold=clustering_threshold,
                    segmentation_threshold=segmentation_threshold,
                    min_speaker_duration=min_speaker_duration,
                    speaker_confidence_threshold=speaker_confidence_threshold,
                    speaker_smoothing_enabled=speaker_smoothing_enabled,
                    min_switch_duration=min_switch_duration,
                    vad_validation_enabled=vad_validation_enabled,
                    optimized_alignment=optimized_alignment,
                    s3_presigned_url=s3_presigned_url,  # Pass to background task for download if needed
                    storage_key=storage_key  # Pass to background task for download if needed
                )
            )

            # Build response message based on configuration
            response_message = f"Transcription job started. Results will be stored locally and retrievable via GET /v1/jobs/{job_id}"
            if JOB_RESULT_RETENTION_SECONDS > 0:
                response_message += f" for {JOB_RESULT_RETENTION_SECONDS / 3600:.1f} hours."
            
            response_data = {
                "job_id": job_id,
                "status": "in_progress",
                "message": response_message,
                "processing_info": {
                    "device": server.device,
                    "language": language,
                    "timestamp_granularities": granularities,
                    "diarization_enabled": enable_diarization,
                    "optimized_alignment": optimized_alignment
                },
                "retrieval_info": {
                    "method": "GET /v1/jobs/{job_id}",
                    "retention_seconds": JOB_RESULT_RETENTION_SECONDS,
                    "note": "Poll this endpoint to check status and retrieve results when complete"
                }
            }
            
            # Add S3 upload info if presigned URL provided
            if upload_presigned_url:
                response_data["callback_info"] = {
                    "upload_url": upload_presigned_url,
                    "note": "Results will also be uploaded to your S3 bucket when complete"
                }
            
            return response_data
        
        # SYNCHRONOUS PROCESSING MODE: Process transcription and wait for completion
        # Run GPU transcription in thread pool to allow concurrent GPU processing
        result = await asyncio.get_event_loop().run_in_executor(
            gpu_executor,
            lambda: server.transcribe_with_diarization(
                temp_audio_path, 
                enable_diarization=enable_diarization,
                language=language,
                model="large-v3-turbo",  # Performance optimized model
                batch_size=batch_size,
                prompt=prompt,
                temperature=0.0,  # Always use 0.0 (ignored parameters handled above)
                timestamp_granularities=timestamp_granularities,
                output_content=output_content,
                clustering_threshold=clustering_threshold,
                segmentation_threshold=segmentation_threshold,
                min_speaker_duration=min_speaker_duration,
                speaker_confidence_threshold=speaker_confidence_threshold,
                speaker_smoothing_enabled=speaker_smoothing_enabled,
                min_switch_duration=min_switch_duration,
                vad_validation_enabled=vad_validation_enabled,
                optimized_alignment=optimized_alignment
            )
        )
        
        # SYNCHRONOUS MODE: Return results inline based on response_format and output_content
        # No storage happens in sync mode - results are returned directly in API response
        # (stored_output=false means sync mode with inline results)
        
        if response_format == "json":
            return result
        elif response_format == "verbose_json":
            return result  # Return as proper JSON response
        elif response_format == "text":
            # Extract just the text content
            text_content = "\n".join([segment.get('text', '') for segment in result.get('result', {}).get('segments', [])])
            return PlainTextResponse(content=text_content.strip())
        elif response_format == "srt":
            # Generate SRT format
            segments = result.get('result', {}).get('segments', [])
            srt_content = ""
            for i, segment in enumerate(segments, 1):
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                text = segment.get('text', '').strip()
                start_srt = f"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{start_time%60:06.3f}".replace('.', ',')
                end_srt = f"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{end_time%60:06.3f}".replace('.', ',')
                srt_content += f"{i}\n{start_srt} --> {end_srt}\n{text}\n\n"
            return PlainTextResponse(content=srt_content.strip())
        elif response_format == "vtt":
            # Generate WebVTT format
            segments = result.get('result', {}).get('segments', [])
            vtt_content = "WEBVTT\n\n"
            for segment in segments:
                start_time = segment.get('start', 0)
                end_time = segment.get('end', 0)
                text = segment.get('text', '').strip()
                start_vtt = f"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{start_time%60:06.3f}"
                end_vtt = f"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{end_time%60:06.3f}"
                vtt_content += f"{start_vtt} --> {end_vtt}\n{text}\n\n"
            return PlainTextResponse(content=vtt_content.strip())
        else:
            # Default: return as JSON
            return result
            
    except Exception as e:
        logger.error(f"Transcription request failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # Cleanup temporary files ONLY in sync mode
        # In async mode, the background task will clean up after processing
        if temp_audio_path and os.path.exists(temp_audio_path) and not stored_output:
            try:
                os.unlink(temp_audio_path)
                logger.debug(f"Cleaned up temporary audio file: {temp_audio_path}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup temporary file {temp_audio_path}: {cleanup_error}")

if __name__ == "__main__":
    logger.info("ðŸš€ Starting WhisperX Diarization Server with Multi-Cloud Storage")
    logger.info("âœ… FEATURES ENABLED:")
    logger.info("   - Advanced speaker diarization with confidence scoring")
    logger.info("   - Multi-cloud storage support (R2, S3)")
    logger.info("   - GPU-accelerated processing")
    logger.info("   - Automatic result export")
    logger.info("ðŸš€ STORAGE INTEGRATIONS:")
    logger.info("   - Cloudflare R2 direct upload and processing")
    logger.info("   - Pre-signed S3 URL support")
    logger.info("   - Automatic result archiving")
    logger.info("   - Zero-bandwidth cost processing for large files")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=3333,
        log_level="info"
    )